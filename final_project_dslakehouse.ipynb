{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8476b62d-2825-400c-be4c-35bb3aa81b1c",
   "metadata": {},
   "source": [
    "# DS-2002 Final Project: Rental Data Lakehouse\n",
    "\n",
    "This project builds a dimensional **Data Lakehouse** using Azure Databricks and PySpark, designed to support the post hoc summarization and analysis of a simple **rental business process**. It integrates batch and streaming data from multiple sources into a unified schema optimized for analysis.\n",
    "\n",
    "### Business Process\n",
    "The pipeline models a movie rental process using the AdventureWorks database as a base. It integrates:\n",
    "\n",
    "- **Customers, Stores, and Products** as dimension tables\n",
    "- **Rental Orders** as the fact table with transaction details\n",
    "\n",
    "### Data Sources\n",
    "- **MySQL**: Core AdventureWorks views (batch)\n",
    "- **MongoDB Atlas**: Store details (semi-structured batch)\n",
    "- **CSV**: Date and Customer dimensions\n",
    "- **Streaming JSON**: Simulated real-time order data in 3 intervals\n",
    "\n",
    "### Architecture Used\n",
    "- **Medallion Architecture**: Bronze → Silver → Gold\n",
    "- **ETL / ELT patterns**\n",
    "- **Structured Streaming** using AutoLoader\n",
    "\n",
    "### Goal\n",
    "Demonstrate value by enabling aggregated queries (e.g., orders by store or customer segment) to support historical business analysis and planning.\n",
    "\n",
    "## Section I: Prerequisites\n",
    "\n",
    "### 1.0. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52688e62-f27c-4772-91c5-29f3b3f32469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pysparkenv/lib/python3.12/site-packages/pyspark\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print(findspark.find())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0263fdaf-8f4a-45e6-b885-1009a1defb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/26 17:16:44 WARN Utils: Your hostname, Sonias-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.0.0.29 instead (on interface en0)\n",
      "25/05/26 17:16:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/26 17:16:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.5.5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Final Project Spark Test\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16355324-fc8a-45b5-b9e5-7b6e8ac814c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pysparkenv/lib/python3.12/site-packages/pyspark\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print(findspark.find())\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pymongo\n",
    "import certifi\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window as W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c2fdf8-2c35-4152-b60a-5e0ae632f60f",
   "metadata": {},
   "source": [
    "### 2.0. Instantiate Final Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7202d396-6d49-46d3-b79e-95fa3275e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Specify MySQL Server Connection Information\n",
    "# --------------------------------------------------------------------------------\n",
    "mysql_args = {\n",
    "    \"host_name\" : \"ds2002-mysql.mysql.database.azure.com\",\n",
    "    \"port\" : \"3306\",\n",
    "    \"db_name\" : \"rental_dw\",\n",
    "    \"conn_props\" : {\n",
    "        \"user\" : \"root\",\n",
    "        \"password\" : \"Carrots123\",\n",
    "        \"driver\" : \"org.mariadb.jdbc.Driver\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify MongoDB Cluster Connection Information\n",
    "# --------------------------------------------------------------------------------\n",
    "mongodb_args = {\n",
    "    \"cluster_location\": \"atlas\",\n",
    "    \"user_name\": \"sonia_sadani\",\n",
    "    \"password\": \"Healthyeating67\",\n",
    "    \"cluster_name\": \"Cluster0\",\n",
    "    \"cluster_subnet\": \"s7ikf\",\n",
    "    \"db_name\": \"northwind_purchasing\",\n",
    "    \"collection\": \"purchase_orders\",  # You can try with \"inventory_transactions\" too\n",
    "    \"null_column_threshold\": 0.5\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify Directory Structure for Source Data\n",
    "# --------------------------------------------------------------------------------\n",
    "base_dir = os.path.join(os.getcwd(), 'lab_data')\n",
    "data_dir = os.path.join(base_dir, 'northwind')\n",
    "batch_dir = os.path.join(base_dir, 'retail-org')\n",
    "stream_dir = os.path.join(data_dir, 'streaming')\n",
    "\n",
    "orders_stream_dir = os.path.join(base_dir, 'retail-org', 'streaming', 'orders')\n",
    "purchase_orders_stream_dir = os.path.join(stream_dir, 'purchase_orders')\n",
    "inventory_trans_stream_dir = os.path.join(stream_dir, 'inventory_transactions')\n",
    "# --------------------------------------------------------------------------------\n",
    "# Create Directory Structure for Data Lakehouse Files\n",
    "# --------------------------------------------------------------------------------\n",
    "dest_database = \"rental_dlh\"\n",
    "sql_warehouse_dir = os.path.abspath('spark-warehouse')\n",
    "dest_database_dir = f\"{dest_database}.db\"\n",
    "database_dir = os.path.join(sql_warehouse_dir, dest_database_dir)\n",
    "\n",
    "orders_output_bronze = os.path.join(database_dir, 'fact_orders', 'bronze')\n",
    "orders_output_silver = os.path.join(database_dir, 'fact_orders', 'silver')\n",
    "orders_output_gold = os.path.join(database_dir, 'fact_orders', 'gold')\n",
    "\n",
    "purchase_orders_output_bronze = os.path.join(database_dir, 'fact_purchase_orders', 'bronze')\n",
    "purchase_orders_output_silver = os.path.join(database_dir, 'fact_purchase_orders', 'silver')\n",
    "purchase_orders_output_gold = os.path.join(database_dir, 'fact_purchase_orders', 'gold')\n",
    "\n",
    "inventory_trans_output_bronze = os.path.join(database_dir, 'fact_inventory_transactions', 'bronze')\n",
    "inventory_trans_output_silver = os.path.join(database_dir, 'fact_inventory_transactions', 'silver')\n",
    "inventory_trans_output_gold = os.path.join(database_dir, 'fact_inventory_transactions', 'gold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021a5185-ad2c-4612-9498-0e2ff4431c5b",
   "metadata": {},
   "source": [
    "### 3.0. Define Final Project Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57c12817-a35b-45ab-92f6-506acb6f1c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_info(path: str):\n",
    "    file_sizes = []\n",
    "    modification_times = []\n",
    "\n",
    "    '''Fetch each item in the directory, and filter out any directories.'''\n",
    "    items = os.listdir(path)\n",
    "    files = sorted([item for item in items if os.path.isfile(os.path.join(path, item))])\n",
    "\n",
    "    '''Populate lists with the Size and Last Modification DateTime for each file in the directory.'''\n",
    "    for file in files:\n",
    "        file_sizes.append(os.path.getsize(os.path.join(path, file)))\n",
    "        modification_times.append(pd.to_datetime(os.path.getmtime(os.path.join(path, file)), unit='s'))\n",
    "\n",
    "    data = list(zip(files, file_sizes, modification_times))\n",
    "    column_names = ['name','size','modification_time']\n",
    "    \n",
    "    return pd.DataFrame(data=data, columns=column_names)\n",
    "\n",
    "\n",
    "def wait_until_stream_is_ready(query, min_batches=1):\n",
    "    while len(query.recentProgress) < min_batches:\n",
    "        time.sleep(5)\n",
    "        \n",
    "    print(f\"The stream has processed {len(query.recentProgress)} batchs\")\n",
    "\n",
    "\n",
    "def remove_directory_tree(path: str):\n",
    "    '''If it exists, remove the entire contents of a directory structure at a given 'path' parameter's location.'''\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "            return f\"Directory '{path}' has been removed successfully.\"\n",
    "        else:\n",
    "            return f\"Directory '{path}' does not exist.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "        \n",
    "\n",
    "def drop_null_columns(df, threshold):\n",
    "    '''Drop Columns having a percentage of NULL values that exceeds the given 'threshold' parameter value.'''\n",
    "    columns_with_nulls = [col for col in df.columns if df.filter(df[col].isNull()).count() / df.count() > threshold] \n",
    "    df_dropped = df.drop(*columns_with_nulls) \n",
    "    \n",
    "    return df_dropped\n",
    "\n",
    "def get_mysql_dataframe(spark_session, sql_query : str, **args):\n",
    "    '''Create a JDBC URL to the MySQL Database'''\n",
    "    jdbc_url = f\"jdbc:mysql://{args['host_name']}:{args['port']}/{args['db_name']}\"\n",
    "    \n",
    "    '''Invoke the spark.read.format(\"jdbc\") function to query the database, and fill a DataFrame.'''\n",
    "    dframe = spark_session.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"driver\", args['conn_props']['driver']) \\\n",
    "    .option(\"user\", args['conn_props']['user']) \\\n",
    "    .option(\"password\", args['conn_props']['password']) \\\n",
    "    .option(\"query\", sql_query) \\\n",
    "    .load()\n",
    "    \n",
    "    return dframe\n",
    "    \n",
    "\n",
    "def get_mongo_uri(**args):\n",
    "    '''Validate proper input'''\n",
    "    if args[\"cluster_location\"] not in ['atlas', 'local']:\n",
    "        raise Exception(\"You must specify either 'atlas' or 'local' for the 'cluster_location' parameter.\")\n",
    "        \n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        uri = f\"mongodb+srv://{args['user_name']}:{args['password']}@\"\n",
    "        uri += f\"{args['cluster_name']}.{args['cluster_subnet']}.mongodb.net/\"\n",
    "    else:\n",
    "        uri = \"mongodb://localhost:27017/\"\n",
    "\n",
    "    return uri\n",
    "\n",
    "\n",
    "def get_spark_conf_args(spark_jars : list, **args):\n",
    "    jars = \"\"\n",
    "    for jar in spark_jars:\n",
    "        jars += f\"{jar}, \"\n",
    "    \n",
    "    sparkConf_args = {\n",
    "        \"app_name\" : \"PySpark Northwind Data Lakehouse (Medallion Architecture)\",\n",
    "        \"worker_threads\" : f\"local[{int(os.cpu_count()/2)}]\",\n",
    "        \"shuffle_partitions\" : int(os.cpu_count()),\n",
    "        \"mongo_uri\" : get_mongo_uri(**args),\n",
    "        \"spark_jars\" : jars[0:-2],\n",
    "        \"database_dir\" : sql_warehouse_dir\n",
    "    }\n",
    "    \n",
    "    return sparkConf_args\n",
    "    \n",
    "\n",
    "def get_spark_conf(**args):\n",
    "    sparkConf = SparkConf().setAppName(args['app_name'])\\\n",
    "    .setMaster(args['worker_threads']) \\\n",
    "    .set('spark.driver.memory', '4g') \\\n",
    "    .set('spark.executor.memory', '2g') \\\n",
    "    .set('spark.jars', args['spark_jars']) \\\n",
    "    .set('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "    .set('spark.mongodb.input.uri', args['mongo_uri']) \\\n",
    "    .set('spark.mongodb.output.uri', args['mongo_uri']) \\\n",
    "    .set('spark.sql.adaptive.enabled', 'false') \\\n",
    "    .set('spark.sql.debug.maxToStringFields', 35) \\\n",
    "    .set('spark.sql.shuffle.partitions', args['shuffle_partitions']) \\\n",
    "    .set('spark.sql.streaming.forceDeleteTempCheckpointLocation', 'true') \\\n",
    "    .set('spark.sql.streaming.schemaInference', 'true') \\\n",
    "    .set('spark.sql.warehouse.dir', args['database_dir']) \\\n",
    "    .set('spark.streaming.stopGracefullyOnShutdown', 'true')\n",
    "    \n",
    "    return sparkConf\n",
    "\n",
    "\n",
    "def get_mongo_client(**args):\n",
    "    '''Get MongoDB Client Connection'''\n",
    "    mongo_uri = get_mongo_uri(**args)\n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        client = pymongo.MongoClient(mongo_uri, tlsCAFile=certifi.where())\n",
    "\n",
    "    elif args['cluster_location'] == \"local\":\n",
    "        client = pymongo.MongoClient(mongo_uri)\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"A MongoDB Client could not be created.\")\n",
    "\n",
    "    return client\n",
    "    \n",
    "    \n",
    "# TODO: Rewrite this to leverage PySpark?\n",
    "def set_mongo_collections(mongo_client, db_name : str, data_directory : str, json_files : list):\n",
    "    db = mongo_client[db_name]\n",
    "    \n",
    "    for file in json_files:\n",
    "        db.drop_collection(file)\n",
    "        json_file = os.path.join(data_directory, json_files[file])\n",
    "        with open(json_file, 'r') as openfile:\n",
    "            json_object = json.load(openfile)\n",
    "            file = db[file]\n",
    "            result = file.insert_many(json_object)\n",
    "        \n",
    "    mongo_client.close()\n",
    "    \n",
    "\n",
    "def get_mongodb_dataframe(spark_session, **args):\n",
    "    '''Query MongoDB Atlas using the modern \"mongodb\" Spark connector format'''\n",
    "    uri = get_mongo_uri(**args)\n",
    "\n",
    "    dframe = spark_session.read.format(\"mongodb\") \\\n",
    "        .option(\"spark.mongodb.read.connection.uri\", uri) \\\n",
    "        .option(\"spark.mongodb.read.database\", args['db_name']) \\\n",
    "        .option(\"spark.mongodb.read.collection\", args['collection']) \\\n",
    "        .load()\n",
    "\n",
    "    dframe = dframe.drop('_id')  # Remove Mongo internal column\n",
    "    dframe = drop_null_columns(dframe, args['null_column_threshold'])  # Optional cleanup\n",
    "\n",
    "    return dframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c929bad4-705f-4a01-8d9d-bba2d84d115c",
   "metadata": {},
   "source": [
    "### 4.0. Initialize Final Project Data Lakehouse Directory Structure                                    Remove the Data Lakehouse Database Directory Structure to Ensure Idempotency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c09080f-afdc-4969-86cc-0fbd0835faf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Directory '/Users/soniasadani/Documents/04-PySpark/spark-warehouse/rental_dlh.db' has been removed successfully.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_directory_tree(database_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa65216-97fc-48d9-b19e-a1f342c53155",
   "metadata": {},
   "source": [
    "### 5.0. Create Spark Session with Final Project Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a416acc5-71ca-40f5-986a-7eeac55b6317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5.0: Create a New Spark Session\n",
    "\n",
    "jars = []\n",
    "\n",
    "# MongoDB Spark Connector (absolute path)\n",
    "mongo_spark_jar = \"/Users/soniasadani/Documents/04-PySpark/mongo-spark-connector_2.12-3.0.1.jar\"\n",
    "jars.append(mongo_spark_jar)\n",
    "\n",
    "# MySQL Connector (absolute path)\n",
    "mysql_spark_jar = os.path.join(os.getcwd(), \"mysql-connector-j-9.1.0\", \"mysql-connector-j-9.1.0.jar\")\n",
    "jars.append(mysql_spark_jar)\n",
    "\n",
    "# Get Spark configuration\n",
    "sparkConf_args = get_spark_conf_args(jars, **mongodb_args)\n",
    "\n",
    "sparkConf = SparkConf().setAppName(\"Data Lakehouse Project\") \\\n",
    "    .setMaster(f\"local[{int(os.cpu_count()/2)}]\") \\\n",
    "    .set(\"spark.driver.memory\", \"4g\") \\\n",
    "    .set(\"spark.executor.memory\", \"2g\") \\\n",
    "    .set(\"spark.jars\", \",\".join(jars)) \\\n",
    "    .set(\"spark.driver.extraClassPath\", \",\".join(jars)) \\\n",
    "    .set(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .set(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .set(\"spark.sql.debug.maxToStringFields\", \"35\") \\\n",
    "    .set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .set(\"spark.sql.streaming.schemaInference\", \"true\") \\\n",
    "    .set(\"spark.sql.warehouse.dir\", sql_warehouse_dir)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"OFF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e8701c-0120-4ba2-81cc-e53ba1f08651",
   "metadata": {},
   "source": [
    "### # 6.0. Create Metadata Database and Load Initial Dimension (Store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "484d76e2-3c95-4d0e-9063-37344aae674c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------------+--------+\n",
      "|address_id|last_update  |manager_staff_id|store_id|\n",
      "+----------+-------------+----------------+--------+\n",
      "|1         |1139979432000|1               |1       |\n",
      "|2         |1139979432000|2               |2       |\n",
      "+----------+-------------+----------------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_path = \"/Users/soniasadani/Documents/04-PySpark/lab_data/retail-org/store_data.json\"\n",
    "df_store_json = spark.read.option(\"multiLine\", \"true\").json(json_path)\n",
    "df_store_json = df_store_json.drop(\"_id\")\n",
    "df_store_json.show(5, truncate=False)\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {dest_database}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ae5f1c-7ec7-408c-bc89-822b7a75b859",
   "metadata": {},
   "source": [
    "## Section II: Populate Dimensions by Ingesting \"Cold-path\" Reference Data \n",
    "### 1.0. Fetch Data from the File System\n",
    "#### 1.1. Verify the location of the source data files on the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c293b98-c631-4342-893b-d174d7cd429b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>modification_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.DS_Store</td>\n",
       "      <td>6148</td>\n",
       "      <td>2025-05-24 17:07:54.204872847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>customers.csv</td>\n",
       "      <td>4550361</td>\n",
       "      <td>2025-03-26 22:08:45.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dim_date.csv</td>\n",
       "      <td>554937</td>\n",
       "      <td>2025-05-20 22:25:30.383128643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>film_data.json</td>\n",
       "      <td>456530</td>\n",
       "      <td>2025-03-27 04:15:43.000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>store_data.json</td>\n",
       "      <td>212</td>\n",
       "      <td>2025-05-23 00:55:57.441878796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name     size             modification_time\n",
       "0        .DS_Store     6148 2025-05-24 17:07:54.204872847\n",
       "1    customers.csv  4550361 2025-03-26 22:08:45.000000000\n",
       "2     dim_date.csv   554937 2025-05-20 22:25:30.383128643\n",
       "3   film_data.json   456530 2025-03-27 04:15:43.000000000\n",
       "4  store_data.json      212 2025-05-23 00:55:57.441878796"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.1. Verify the location of the source data files on the file system\n",
    "cold_path_dir = os.path.join(base_dir, 'retail-org')\n",
    "get_file_info(cold_path_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e13a92a-2e5e-46b3-babc-ac10f20d35da",
   "metadata": {},
   "source": [
    "#### 1.2. Populate the <span style=\"color:darkred\">Customers Dimension</span>\n",
    "##### 1.2.1. Use PySpark to Read data from a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63f9eac0-5c92-4a98-8455-8be2a034d935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/soniasadani/Documents/04-PySpark/lab_data/retail-org/customers.csv\n",
      "+-----------+------+--------+----------------------+-----+-------+--------+-----------+------+----+-------+--------+------------+----------+-------------------------------+----------+-------------+---------------+---------------+\n",
      "|customer_id|tax_id|tax_code|customer_name         |state|city   |postcode|street     |number|unit|region |district|lon         |lat       |ship_to_address                |valid_from|valid_to     |units_purchased|loyalty_segment|\n",
      "+-----------+------+--------+----------------------+-----+-------+--------+-----------+------+----+-------+--------+------------+----------+-------------------------------+----------+-------------+---------------+---------------+\n",
      "|11123757   |NULL  |NULL    |SMITH,  SHIRLEY       |IN   |BREMEN |46506.0 |N CENTER ST|521.0 |NULL|Indiana|50.0    |-86.1465825 |41.4507625|IN, 46506.0, N CENTER ST, 521.0|1532824233|1.548137353E9|34.0           |3              |\n",
      "|30585978   |NULL  |NULL    |STEPHENS,  GERALDINE M|OR   |ADDRESS|0       |NO SITUS   |NULL  |NULL|NULL   |NULL    |-122.1055158|45.374317 |OR, 0, NO SITUS, nan           |1523100473|NULL         |18.0           |3              |\n",
      "+-----------+------+--------+----------------------+-----+-------+--------+-----------+------+----+-------+--------+------------+----------+-------------------------------+----------+-------------+---------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_csv = os.path.join(batch_dir, 'customers.csv')\n",
    "print(customer_csv)\n",
    "\n",
    "df_dim_customers = spark.read.format('csv') \\\n",
    "    .options(header='true', inferSchema='true') \\\n",
    "    .load(customer_csv)\n",
    "\n",
    "df_dim_customers.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e9aedf-9033-403d-a802-b09be878865f",
   "metadata": {},
   "source": [
    "##### 1.2.2. Transform the Customers Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4334bc4c-b70f-4e61-b65a-76eced4c77f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['customer_id', 'tax_id', 'tax_code', 'customer_name', 'state', 'city', 'postcode', 'street', 'number', 'unit', 'region', 'district', 'lon', 'lat', 'ship_to_address', 'valid_from', 'valid_to', 'units_purchased', 'loyalty_segment', 'customer_key']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_key</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>customer_name</th>\n",
       "      <th>tax_id</th>\n",
       "      <th>tax_code</th>\n",
       "      <th>street</th>\n",
       "      <th>number</th>\n",
       "      <th>unit</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postcode</th>\n",
       "      <th>region</th>\n",
       "      <th>district</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>loyalty_segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1668</td>\n",
       "      <td>NGUYEN,  LINH THI MY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>COLBY RD</td>\n",
       "      <td>1360</td>\n",
       "      <td>None</td>\n",
       "      <td>LUNENBURG</td>\n",
       "      <td>VT</td>\n",
       "      <td>05906</td>\n",
       "      <td>VT</td>\n",
       "      <td>ESSEX</td>\n",
       "      <td>-71.673995</td>\n",
       "      <td>44.513836</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2405</td>\n",
       "      <td>intel security</td>\n",
       "      <td>766739662.0</td>\n",
       "      <td>A</td>\n",
       "      <td>HARVEY ST</td>\n",
       "      <td>83</td>\n",
       "      <td>None</td>\n",
       "      <td>SAINT JOHNSBURY</td>\n",
       "      <td>VT</td>\n",
       "      <td>05819</td>\n",
       "      <td>VT</td>\n",
       "      <td>None</td>\n",
       "      <td>-72.023713</td>\n",
       "      <td>44.425748</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_key  customer_id         customer_name       tax_id tax_code  \\\n",
       "0             1         1668  NGUYEN,  LINH THI MY          NaN     None   \n",
       "1             2         2405        intel security  766739662.0        A   \n",
       "\n",
       "      street number  unit             city state postcode region district  \\\n",
       "0   COLBY RD   1360  None        LUNENBURG    VT    05906     VT    ESSEX   \n",
       "1  HARVEY ST     83  None  SAINT JOHNSBURY    VT    05819     VT     None   \n",
       "\n",
       "         lon        lat  loyalty_segment  \n",
       "0 -71.673995  44.513836                0  \n",
       "1 -72.023713  44.425748                0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename 'id' column if needed (not applicable here since it's 'customer_id')\n",
    "# Add surrogate key\n",
    "df_dim_customers.createOrReplaceTempView(\"customers\")\n",
    "sql_customers = \"\"\"\n",
    "    SELECT *, ROW_NUMBER() OVER (ORDER BY customer_id) AS customer_key\n",
    "    FROM customers\n",
    "\"\"\"\n",
    "df_dim_customers = spark.sql(sql_customers)\n",
    "\n",
    "# Choose and reorder relevant columns\n",
    "ordered_columns = ['customer_key', 'customer_id', 'customer_name', 'tax_id', 'tax_code',\n",
    "                   'street', 'number', 'unit', 'city', 'state', 'postcode', 'country_region',\n",
    "                   'region', 'district', 'lon', 'lat', 'loyalty_segment']\n",
    "\n",
    "# Some of these might still be missing — so run:\n",
    "print(df_dim_customers.columns)\n",
    "# Then remove any that don't match before running the next line\n",
    "\n",
    "df_dim_customers = df_dim_customers.select([col for col in ordered_columns if col in df_dim_customers.columns])\n",
    "df_dim_customers.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043a826b-085a-42ce-b9b1-f0991e578eb6",
   "metadata": {},
   "source": [
    "##### 1.2.3. Save as the `dim_customers` Table in the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5931ed64-99f7-4281-b903-a052c89ce6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_dim_customers.write.saveAsTable(f\"{dest_database}.dim_customers\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7eacbe-1cad-4ded-9b98-814d140ed8d6",
   "metadata": {},
   "source": [
    "##### 1.2.4. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "414f418e-b9aa-4ef0-a256-b7dd3de1519b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------+-------+\n",
      "|col_name                    |data_type    |comment|\n",
      "+----------------------------+-------------+-------+\n",
      "|customer_key                |int          |NULL   |\n",
      "|customer_id                 |int          |NULL   |\n",
      "|customer_name               |string       |NULL   |\n",
      "|tax_id                      |double       |NULL   |\n",
      "|tax_code                    |string       |NULL   |\n",
      "|street                      |string       |NULL   |\n",
      "|number                      |string       |NULL   |\n",
      "|unit                        |string       |NULL   |\n",
      "|city                        |string       |NULL   |\n",
      "|state                       |string       |NULL   |\n",
      "|postcode                    |string       |NULL   |\n",
      "|region                      |string       |NULL   |\n",
      "|district                    |string       |NULL   |\n",
      "|lon                         |double       |NULL   |\n",
      "|lat                         |double       |NULL   |\n",
      "|loyalty_segment             |int          |NULL   |\n",
      "|                            |             |       |\n",
      "|# Detailed Table Information|             |       |\n",
      "|Catalog                     |spark_catalog|       |\n",
      "|Database                    |rental_dlh   |       |\n",
      "+----------------------------+-------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_key</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>customer_name</th>\n",
       "      <th>tax_id</th>\n",
       "      <th>tax_code</th>\n",
       "      <th>street</th>\n",
       "      <th>number</th>\n",
       "      <th>unit</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postcode</th>\n",
       "      <th>region</th>\n",
       "      <th>district</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>loyalty_segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1668</td>\n",
       "      <td>NGUYEN,  LINH THI MY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>COLBY RD</td>\n",
       "      <td>1360</td>\n",
       "      <td>None</td>\n",
       "      <td>LUNENBURG</td>\n",
       "      <td>VT</td>\n",
       "      <td>05906</td>\n",
       "      <td>VT</td>\n",
       "      <td>ESSEX</td>\n",
       "      <td>-71.673995</td>\n",
       "      <td>44.513836</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2405</td>\n",
       "      <td>intel security</td>\n",
       "      <td>766739662.0</td>\n",
       "      <td>A</td>\n",
       "      <td>HARVEY ST</td>\n",
       "      <td>83</td>\n",
       "      <td>None</td>\n",
       "      <td>SAINT JOHNSBURY</td>\n",
       "      <td>VT</td>\n",
       "      <td>05819</td>\n",
       "      <td>VT</td>\n",
       "      <td>None</td>\n",
       "      <td>-72.023713</td>\n",
       "      <td>44.425748</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_key  customer_id         customer_name       tax_id tax_code  \\\n",
       "0             1         1668  NGUYEN,  LINH THI MY          NaN     None   \n",
       "1             2         2405        intel security  766739662.0        A   \n",
       "\n",
       "      street number  unit             city state postcode region district  \\\n",
       "0   COLBY RD   1360  None        LUNENBURG    VT    05906     VT    ESSEX   \n",
       "1  HARVEY ST     83  None  SAINT JOHNSBURY    VT    05819     VT     None   \n",
       "\n",
       "         lon        lat  loyalty_segment  \n",
       "0 -71.673995  44.513836                0  \n",
       "1 -72.023713  44.425748                0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Unit Test: Describe and Preview dim_customers table ===\n",
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_customers;\").show(truncate=False)\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_customers LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c1854d-8ea5-421d-b2d7-62109d67581f",
   "metadata": {},
   "source": [
    "### 2.0. Fetch Reference Data from a MongoDB Atlas Database\n",
    "#### 2.1. Create a New MongoDB Database, and Load Each JSON File into a New MongoDB Collection\n",
    "**NOTE:** The following cell **can** be run more than once because the **set_mongo_collection()** function **is** idempotent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df1e169c-3bd8-4f09-9b96-66e6126b8af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------------+----------+-------------+\n",
      "|store_key|store_id|manager_staff_id|address_id|last_update  |\n",
      "+---------+--------+----------------+----------+-------------+\n",
      "|1        |1       |1               |1         |1139979432000|\n",
      "|2        |2       |2               |2         |1139979432000|\n",
      "+---------+--------+----------------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === Section 2.0: Simulate MongoDB Reference Data ===\n",
    "# Instead of connecting to MongoDB Atlas, load the store_data.json file directly for the final project\n",
    "\n",
    "# Load the JSON data\n",
    "json_path = \"/Users/soniasadani/Documents/04-PySpark/lab_data/retail-org/store_data.json\"\n",
    "\n",
    "df_dim_stores = spark.read.option(\"multiLine\", \"true\").json(json_path)\n",
    "df_dim_stores = df_dim_stores.drop(\"_id\")  # Just in case it exists\n",
    "\n",
    "# Add surrogate key\n",
    "df_dim_stores.createOrReplaceTempView(\"stores\")\n",
    "sql_stores = \"\"\"\n",
    "    SELECT *, ROW_NUMBER() OVER (ORDER BY store_id) AS store_key\n",
    "    FROM stores\n",
    "\"\"\"\n",
    "df_dim_stores = spark.sql(sql_stores)\n",
    "\n",
    "# Reorder columns based on real schema\n",
    "ordered_columns = ['store_key', 'store_id', 'manager_staff_id', 'address_id', 'last_update']\n",
    "df_dim_stores = df_dim_stores.select(*ordered_columns)\n",
    "\n",
    "# Preview result\n",
    "df_dim_stores.show(2, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59029949-2103-40b8-8393-3707e0e83846",
   "metadata": {},
   "source": [
    "##### 2.2.2. Make Necessary Transformations to the New Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a150b003-7d92-421e-9a27-e018c7711908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------------+----------+-------------+\n",
      "|store_key|store_id|manager_staff_id|address_id|last_update  |\n",
      "+---------+--------+----------------+----------+-------------+\n",
      "|1        |1       |1               |1         |1139979432000|\n",
      "|2        |2       |2               |2         |1139979432000|\n",
      "+---------+--------+----------------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Rename ID column\n",
    "df_dim_stores = df_dim_stores.withColumnRenamed(\"id\", \"store_id\")\n",
    "\n",
    "# Add surrogate primary key\n",
    "df_dim_stores = df_dim_stores.withColumn(\n",
    "    \"store_key\",\n",
    "    row_number().over(Window.orderBy(\"store_id\"))\n",
    ")\n",
    "\n",
    "# Reorder columns (put store_key first)\n",
    "cols = [\"store_key\"] + [col for col in df_dim_stores.columns if col != \"store_key\"]\n",
    "df_dim_stores = df_dim_stores.select(cols)\n",
    "\n",
    "# Optional: Preview\n",
    "df_dim_stores.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c71d4b7-b620-49db-ba43-0b5c83b77b61",
   "metadata": {},
   "source": [
    "##### 2.2.3. Save as the <span style=\"color:darkred\">df_dim_stores</span> table in the Data lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01482130-e523-4e74-8431-c7565e714be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------------+----------+-------------+\n",
      "|store_key|store_id|manager_staff_id|address_id|last_update  |\n",
      "+---------+--------+----------------+----------+-------------+\n",
      "|1        |1       |1               |1         |1139979432000|\n",
      "|2        |2       |2               |2         |1139979432000|\n",
      "+---------+--------+----------------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dim_stores.write.saveAsTable(f\"{dest_database}.dim_stores\", mode=\"overwrite\")\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_stores LIMIT 5\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d8d82a-dba4-40e3-aecc-0f4763b94679",
   "metadata": {},
   "source": [
    "##### 2.2.4: Validate Store Dimension Table Structure and Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64add729-9a97-483c-874e-c5aede8eadc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                            |comment|\n",
      "+----------------------------+-------------------------------------------------------------------------------------+-------+\n",
      "|store_key                   |int                                                                                  |NULL   |\n",
      "|store_id                    |bigint                                                                               |NULL   |\n",
      "|manager_staff_id            |bigint                                                                               |NULL   |\n",
      "|address_id                  |bigint                                                                               |NULL   |\n",
      "|last_update                 |bigint                                                                               |NULL   |\n",
      "|                            |                                                                                     |       |\n",
      "|# Detailed Table Information|                                                                                     |       |\n",
      "|Catalog                     |spark_catalog                                                                        |       |\n",
      "|Database                    |rental_dlh                                                                           |       |\n",
      "|Table                       |dim_stores                                                                           |       |\n",
      "|Created Time                |Mon May 26 17:16:51 EDT 2025                                                         |       |\n",
      "|Last Access                 |UNKNOWN                                                                              |       |\n",
      "|Created By                  |Spark 3.5.5                                                                          |       |\n",
      "|Type                        |MANAGED                                                                              |       |\n",
      "|Provider                    |parquet                                                                              |       |\n",
      "|Location                    |file:/Users/soniasadani/Documents/04-PySpark/spark-warehouse/rental_dlh.db/dim_stores|       |\n",
      "+----------------------------+-------------------------------------------------------------------------------------+-------+\n",
      "\n",
      "+---------+--------+----------------+----------+-------------+\n",
      "|store_key|store_id|manager_staff_id|address_id|last_update  |\n",
      "+---------+--------+----------------+----------+-------------+\n",
      "|1        |1       |1               |1         |1139979432000|\n",
      "|2        |2       |2               |2         |1139979432000|\n",
      "+---------+--------+----------------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the table structure\n",
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_stores\").show(truncate=False)\n",
    "\n",
    "# Preview first 5 rows\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_stores LIMIT 5\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11cf43d-c4be-446a-bdf9-73400fab4561",
   "metadata": {},
   "source": [
    "##### 2.4. Ingest and Transform Store Dimension (Simulated MongoDB JSON)\n",
    "##### 2.4.1. Load Store JSON Data into Spark DataFrame <span style=\"color:darkred\">Invoices</span> Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c2f01d2-1a8e-445a-9913-549b06c783ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading store data from: /Users/soniasadani/Documents/04-PySpark/lab_data/retail-org/store_data.json\n",
      "root\n",
      " |-- address_id: long (nullable = true)\n",
      " |-- last_update: long (nullable = true)\n",
      " |-- manager_staff_id: long (nullable = true)\n",
      " |-- store_id: long (nullable = true)\n",
      "\n",
      "+----------+-------------+----------------+--------+\n",
      "|address_id|last_update  |manager_staff_id|store_id|\n",
      "+----------+-------------+----------------+--------+\n",
      "|1         |1139979432000|1               |1       |\n",
      "|2         |1139979432000|2               |2       |\n",
      "+----------+-------------+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === Load store_data.json as replacement for MongoDB store dimension ===\n",
    "store_json = os.path.join(batch_dir, 'store_data.json')\n",
    "print(f\"Loading store data from: {store_json}\")\n",
    "\n",
    "# Load JSON\n",
    "df_dim_stores = (\n",
    "    spark.read\n",
    "         .option(\"multiLine\", True)\n",
    "         .json(store_json)\n",
    ")\n",
    "\n",
    "# Print schema and preview\n",
    "df_dim_stores.printSchema()\n",
    "df_dim_stores.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe339fa3-907d-4e5a-9f72-3cd71e179a84",
   "metadata": {},
   "source": [
    "##### 2.4.2. Apply Transformations and Add Surrogate Key to Store Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8e7dc00b-1241-4463-9d07-35066896cba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------------+----------------+--------+\n",
      "|store_key|address_id|last_update  |manager_staff_id|store_id|\n",
      "+---------+----------+-------------+----------------+--------+\n",
      "|1        |1         |1139979432000|1               |1       |\n",
      "|2        |2         |1139979432000|2               |2       |\n",
      "+---------+----------+-------------+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# --- Rename 'id' to 'store_id' if needed (depends on your JSON structure) ---\n",
    "df_dim_stores = df_dim_stores.withColumnRenamed(\"id\", \"store_id\")\n",
    "\n",
    "# --- Add surrogate key ---\n",
    "window_spec = Window.orderBy(\"store_id\")\n",
    "df_dim_stores = df_dim_stores.withColumn(\"store_key\", row_number().over(window_spec))\n",
    "\n",
    "# --- Reorder columns (adjust based on your actual schema) ---\n",
    "cols = [\"store_key\"] + [col for col in df_dim_stores.columns if col != \"store_key\"]\n",
    "df_dim_stores = df_dim_stores.select(cols)\n",
    "\n",
    "# --- Preview ---\n",
    "df_dim_stores.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c3fc36-487a-4342-b47f-c2cbc04f27b4",
   "metadata": {},
   "source": [
    "##### 2.4.3. Save Store Dimension as dim_stores Table in the Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "76518f9d-133a-4c8a-969b-6ed014fd028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_stores.write.saveAsTable(f\"{dest_database}.dim_stores\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e9d072-764d-478e-80d7-f5b84f6618b3",
   "metadata": {},
   "source": [
    "##### 2.4.4. Validate Store Dimension Table Structure and Preview Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8b046075-3b8f-45fb-a5ce-235862429603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+-------+\n",
      "|col_name        |data_type|comment|\n",
      "+----------------+---------+-------+\n",
      "|store_key       |int      |NULL   |\n",
      "|address_id      |bigint   |NULL   |\n",
      "|last_update     |bigint   |NULL   |\n",
      "|manager_staff_id|bigint   |NULL   |\n",
      "|store_id        |bigint   |NULL   |\n",
      "+----------------+---------+-------+\n",
      "\n",
      "+---------+----------+-------------+----------------+--------+\n",
      "|store_key|address_id|last_update  |manager_staff_id|store_id|\n",
      "+---------+----------+-------------+----------------+--------+\n",
      "|1        |1         |1139979432000|1               |1       |\n",
      "|2        |2         |1139979432000|2               |2       |\n",
      "+---------+----------+-------------+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE {dest_database}.dim_stores\").show(truncate=False)\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_stores LIMIT 5\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8947777-e4a6-461f-9490-301ccaf4b06a",
   "metadata": {},
   "source": [
    "### 3.0. Ingest and Transform Language Dimension (MySQL Source)\n",
    "#### 3.1. Read Language Table from MySQL Data Warehouse\n",
    "##### 3.1.1. Use Spark JDBC to Load language Table from MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0f4af01d-9f24-4287-87cd-07fc31502958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dim_date from: /Users/soniasadani/Documents/04-PySpark/lab_data/retail-org/dim_date.csv\n",
      "root\n",
      " |-- date_key: integer (nullable = true)\n",
      " |-- full_date: date (nullable = true)\n",
      " |-- date_name: string (nullable = true)\n",
      " |-- date_name_us: string (nullable = true)\n",
      " |-- date_name_eu: string (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- day_name_of_week: string (nullable = true)\n",
      " |-- day_of_month: integer (nullable = true)\n",
      " |-- day_of_year: integer (nullable = true)\n",
      " |-- weekday_weekend: string (nullable = true)\n",
      " |-- week_of_year: integer (nullable = true)\n",
      " |-- month_name: string (nullable = true)\n",
      " |-- month_of_year: integer (nullable = true)\n",
      " |-- is_last_day_of_month: string (nullable = true)\n",
      " |-- calendar_quarter: integer (nullable = true)\n",
      " |-- calendar_year: integer (nullable = true)\n",
      " |-- calendar_year_month: timestamp (nullable = true)\n",
      " |-- calendar_year_qtr: string (nullable = true)\n",
      " |-- fiscal_month_of_year: integer (nullable = true)\n",
      " |-- fiscal_quarter: integer (nullable = true)\n",
      " |-- fiscal_year: integer (nullable = true)\n",
      " |-- fiscal_year_month: timestamp (nullable = true)\n",
      " |-- fiscal_year_qtr: string (nullable = true)\n",
      "\n",
      "+--------+----------+----------+------------+------------+-----------+----------------+------------+-----------+---------------+------------+----------+-------------+--------------------+----------------+-------------+-------------------+-----------------+--------------------+--------------+-----------+-------------------+---------------+\n",
      "|date_key|full_date |date_name |date_name_us|date_name_eu|day_of_week|day_name_of_week|day_of_month|day_of_year|weekday_weekend|week_of_year|month_name|month_of_year|is_last_day_of_month|calendar_quarter|calendar_year|calendar_year_month|calendar_year_qtr|fiscal_month_of_year|fiscal_quarter|fiscal_year|fiscal_year_month  |fiscal_year_qtr|\n",
      "+--------+----------+----------+------------+------------+-----------+----------------+------------+-----------+---------------+------------+----------+-------------+--------------------+----------------+-------------+-------------------+-----------------+--------------------+--------------+-----------+-------------------+---------------+\n",
      "|20000101|2000-01-01|2000/01/01|01/01/2000  |01/01/2000  |7          |Saturday        |1           |1          |Weekend        |52          |January   |1            |N                   |1               |2000         |2000-01-01 00:00:00|2000Q1           |7                   |3             |2000       |2000-07-01 00:00:00|2000Q3         |\n",
      "|20000102|2000-01-02|2000/01/02|01/02/2000  |02/01/2000  |1          |Sunday          |2           |2          |Weekend        |52          |January   |1            |N                   |1               |2000         |2000-01-01 00:00:00|2000Q1           |7                   |3             |2000       |2000-07-01 00:00:00|2000Q3         |\n",
      "|20000103|2000-01-03|2000/01/03|01/03/2000  |03/01/2000  |2          |Monday          |3           |3          |Weekday        |1           |January   |1            |N                   |1               |2000         |2000-01-01 00:00:00|2000Q1           |7                   |3             |2000       |2000-07-01 00:00:00|2000Q3         |\n",
      "|20000104|2000-01-04|2000/01/04|01/04/2000  |04/01/2000  |3          |Tuesday         |4           |4          |Weekday        |1           |January   |1            |N                   |1               |2000         |2000-01-01 00:00:00|2000Q1           |7                   |3             |2000       |2000-07-01 00:00:00|2000Q3         |\n",
      "|20000105|2000-01-05|2000/01/05|01/05/2000  |05/01/2000  |4          |Wednesday       |5           |5          |Weekday        |1           |January   |1            |N                   |1               |2000         |2000-01-01 00:00:00|2000Q1           |7                   |3             |2000       |2000-07-01 00:00:00|2000Q3         |\n",
      "+--------+----------+----------+------------+------------+-----------+----------------+------------+-----------+---------------+------------+----------+-------------+--------------------+----------------+-------------+-------------------+-----------------+--------------------+--------------+-----------+-------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#I was having this driver Java error and couldn't fix it\n",
    "#I don't know why this driver's not working, because it was working for the stuff I did before \n",
    "#So, I loaded them in locally as CSV files\n",
    "\n",
    "# Set the path to the date dimension CSV (adjust path if needed)\n",
    "dim_date_csv = os.path.join(batch_dir, 'dim_date.csv')\n",
    "print(f\"Loading dim_date from: {dim_date_csv}\")\n",
    "\n",
    "# Load into Spark\n",
    "df_dim_date = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)\n",
    "         .option(\"inferSchema\", True)\n",
    "         .csv(dim_date_csv)\n",
    ")\n",
    "\n",
    "# Preview\n",
    "df_dim_date.printSchema()\n",
    "df_dim_date.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23be56fc-b6e3-42e1-801d-3474957aee4a",
   "metadata": {},
   "source": [
    "##### 3.1.2. Save as the <span style=\"color:darkred\">dim_date</span> table in the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "868fc437-5f13-447e-8def-1fc663eb05a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_date.write.saveAsTable(f\"{dest_database}.dim_date\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f64095-6b91-43b4-b96f-20d4fafa9a35",
   "metadata": {},
   "source": [
    "##### 3.1.3. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a08f8ad8-b44d-4a82-bd4e-d94757964c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|col_name            |data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|date_key            |int      |NULL   |\n",
      "|full_date           |date     |NULL   |\n",
      "|date_name           |string   |NULL   |\n",
      "|date_name_us        |string   |NULL   |\n",
      "|date_name_eu        |string   |NULL   |\n",
      "|day_of_week         |int      |NULL   |\n",
      "|day_name_of_week    |string   |NULL   |\n",
      "|day_of_month        |int      |NULL   |\n",
      "|day_of_year         |int      |NULL   |\n",
      "|weekday_weekend     |string   |NULL   |\n",
      "|week_of_year        |int      |NULL   |\n",
      "|month_name          |string   |NULL   |\n",
      "|month_of_year       |int      |NULL   |\n",
      "|is_last_day_of_month|string   |NULL   |\n",
      "|calendar_quarter    |int      |NULL   |\n",
      "|calendar_year       |int      |NULL   |\n",
      "|calendar_year_month |timestamp|NULL   |\n",
      "|calendar_year_qtr   |string   |NULL   |\n",
      "|fiscal_month_of_year|int      |NULL   |\n",
      "|fiscal_quarter      |int      |NULL   |\n",
      "+--------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+----------+----------+------------+------------+-----------+----------------+------------+-----------+---------------+------------+----------+-------------+--------------------+----------------+-------------+-------------------+-----------------+--------------------+--------------+-----------+-------------------+---------------+\n",
      "|date_key|full_date |date_name |date_name_us|date_name_eu|day_of_week|day_name_of_week|day_of_month|day_of_year|weekday_weekend|week_of_year|month_name|month_of_year|is_last_day_of_month|calendar_quarter|calendar_year|calendar_year_month|calendar_year_qtr|fiscal_month_of_year|fiscal_quarter|fiscal_year|fiscal_year_month  |fiscal_year_qtr|\n",
      "+--------+----------+----------+------------+------------+-----------+----------------+------------+-----------+---------------+------------+----------+-------------+--------------------+----------------+-------------+-------------------+-----------------+--------------------+--------------+-----------+-------------------+---------------+\n",
      "|20000101|2000-01-01|2000/01/01|01/01/2000  |01/01/2000  |7          |Saturday        |1           |1          |Weekend        |52          |January   |1            |N                   |1               |2000         |2000-01-01 00:00:00|2000Q1           |7                   |3             |2000       |2000-07-01 00:00:00|2000Q3         |\n",
      "|20000102|2000-01-02|2000/01/02|01/02/2000  |02/01/2000  |1          |Sunday          |2           |2          |Weekend        |52          |January   |1            |N                   |1               |2000         |2000-01-01 00:00:00|2000Q1           |7                   |3             |2000       |2000-07-01 00:00:00|2000Q3         |\n",
      "|20000103|2000-01-03|2000/01/03|01/03/2000  |03/01/2000  |2          |Monday          |3           |3          |Weekday        |1           |January   |1            |N                   |1               |2000         |2000-01-01 00:00:00|2000Q1           |7                   |3             |2000       |2000-07-01 00:00:00|2000Q3         |\n",
      "|20000104|2000-01-04|2000/01/04|01/04/2000  |04/01/2000  |3          |Tuesday         |4           |4          |Weekday        |1           |January   |1            |N                   |1               |2000         |2000-01-01 00:00:00|2000Q1           |7                   |3             |2000       |2000-07-01 00:00:00|2000Q3         |\n",
      "|20000105|2000-01-05|2000/01/05|01/05/2000  |05/01/2000  |4          |Wednesday       |5           |5          |Weekday        |1           |January   |1            |N                   |1               |2000         |2000-01-01 00:00:00|2000Q1           |7                   |3             |2000       |2000-07-01 00:00:00|2000Q3         |\n",
      "+--------+----------+----------+------------+------------+-----------+----------------+------------+-----------+---------------+------------+----------+-------------+--------------------+----------------+-------------+-------------------+-----------------+--------------------+--------------+-----------+-------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_date\").show(truncate=False)\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_date LIMIT 5\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df0a18f-6a72-4b02-b4a7-f41080d0ca4d",
   "metadata": {},
   "source": [
    "#### 3.2. Populate the <span style=\"color:darkred\">Product Dimension</span>\n",
    "##### 3.2.1. Fetch data from the <span style=\"color:darkred\">Products</span> table in MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ff551005-e91d-4dd4-9b2a-36326b0d73ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading products from: /Users/soniasadani/Documents/04-PySpark/lab_data/retail-org/film_data.json\n",
      "root\n",
      " |-- description: string (nullable = true)\n",
      " |-- film_id: long (nullable = true)\n",
      " |-- language_id: long (nullable = true)\n",
      " |-- last_update: long (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- original_language_id: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- release_year: long (nullable = true)\n",
      " |-- rental_duration: long (nullable = true)\n",
      " |-- rental_rate: double (nullable = true)\n",
      " |-- replacement_cost: double (nullable = true)\n",
      " |-- special_features: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n",
      "+---------------------------------------------------------------------------------------------------------------------+-------+-----------+-------------+------+--------------------+------+------------+---------------+-----------+----------------+--------------------------------+----------------+\n",
      "|description                                                                                                          |film_id|language_id|last_update  |length|original_language_id|rating|release_year|rental_duration|rental_rate|replacement_cost|special_features                |title           |\n",
      "+---------------------------------------------------------------------------------------------------------------------+-------+-----------+-------------+------+--------------------+------+------------+---------------+-----------+----------------+--------------------------------+----------------+\n",
      "|A Epic Drama of a Feminist And a Mad Scientist who must Battle a Teacher in The Canadian Rockies                     |1      |1          |1139979822000|86    |NULL                |PG    |2006        |6              |0.99       |20.99           |Deleted Scenes,Behind the Scenes|ACADEMY DINOSAUR|\n",
      "|A Astounding Epistle of a Database Administrator And a Explorer who must Find a Car in Ancient China                 |2      |1          |1139979822000|48    |NULL                |G     |2006        |3              |4.99       |12.99           |Trailers,Deleted Scenes         |ACE GOLDFINGER  |\n",
      "|A Astounding Reflection of a Lumberjack And a Car who must Sink a Lumberjack in A Baloon Factory                     |3      |1          |1139979822000|50    |NULL                |NC-17 |2006        |7              |2.99       |18.99           |Trailers,Deleted Scenes         |ADAPTATION HOLES|\n",
      "|A Fanciful Documentary of a Frisbee And a Lumberjack who must Chase a Monkey in A Shark Tank                         |4      |1          |1139979822000|117   |NULL                |G     |2006        |5              |2.99       |26.99           |Commentaries,Behind the Scenes  |AFFAIR PREJUDICE|\n",
      "|A Fast-Paced Documentary of a Pastry Chef And a Dentist who must Pursue a Forensic Psychologist in The Gulf of Mexico|5      |1          |1139979822000|130   |NULL                |G     |2006        |6              |2.99       |22.99           |Deleted Scenes                  |AFRICAN EGG     |\n",
      "+---------------------------------------------------------------------------------------------------------------------+-------+-----------+-------------+------+--------------------+------+------------+---------------+-----------+----------------+--------------------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.2.1 Fetch data from film_data.json (Product Dimension)\n",
    "\n",
    "# Set the correct path to your JSON file\n",
    "products_json = os.path.join(batch_dir, 'film_data.json')\n",
    "print(f\"Loading products from: {products_json}\")\n",
    "\n",
    "# Load into Spark DataFrame\n",
    "df_dim_products = (\n",
    "    spark.read\n",
    "         .option(\"multiLine\", True)\n",
    "         .json(products_json)\n",
    ")\n",
    "\n",
    "# Preview structure and content\n",
    "df_dim_products.printSchema()\n",
    "df_dim_products.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b715baa-4e46-40dd-9cd1-eeafbaefa977",
   "metadata": {},
   "source": [
    "##### 3.2.2. Perform any Necessary Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "39abd0ce-d5ea-4204-b4f3-95cec0d2bb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_key</th>\n",
       "      <th>product_id</th>\n",
       "      <th>language_id</th>\n",
       "      <th>last_update</th>\n",
       "      <th>length</th>\n",
       "      <th>original_language_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>release_year</th>\n",
       "      <th>rental_duration</th>\n",
       "      <th>rental_rate</th>\n",
       "      <th>replacement_cost</th>\n",
       "      <th>special_features</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1139979822000</td>\n",
       "      <td>86</td>\n",
       "      <td>None</td>\n",
       "      <td>PG</td>\n",
       "      <td>2006</td>\n",
       "      <td>6</td>\n",
       "      <td>0.99</td>\n",
       "      <td>20.99</td>\n",
       "      <td>Deleted Scenes,Behind the Scenes</td>\n",
       "      <td>ACADEMY DINOSAUR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1139979822000</td>\n",
       "      <td>48</td>\n",
       "      <td>None</td>\n",
       "      <td>G</td>\n",
       "      <td>2006</td>\n",
       "      <td>3</td>\n",
       "      <td>4.99</td>\n",
       "      <td>12.99</td>\n",
       "      <td>Trailers,Deleted Scenes</td>\n",
       "      <td>ACE GOLDFINGER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_key  product_id  language_id    last_update  length  \\\n",
       "0            1           1            1  1139979822000      86   \n",
       "1            2           2            1  1139979822000      48   \n",
       "\n",
       "  original_language_id rating  release_year  rental_duration  rental_rate  \\\n",
       "0                 None     PG          2006                6         0.99   \n",
       "1                 None      G          2006                3         4.99   \n",
       "\n",
       "   replacement_cost                  special_features             title  \n",
       "0             20.99  Deleted Scenes,Behind the Scenes  ACADEMY DINOSAUR  \n",
       "1             12.99           Trailers,Deleted Scenes    ACE GOLDFINGER  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Rename the 'film_id' column to 'product_id' for consistency\n",
    "df_dim_products = df_dim_products.withColumnRenamed(\"film_id\", \"product_id\")\n",
    "\n",
    "# Drop columns not needed (update if needed)\n",
    "columns_to_drop = ['description', 'attachments']  # Remove if these don't exist\n",
    "existing_cols = df_dim_products.columns\n",
    "df_dim_products = df_dim_products.drop(*[col for col in columns_to_drop if col in existing_cols])\n",
    "\n",
    "# Add surrogate primary key\n",
    "window_spec = Window.orderBy(\"product_id\")\n",
    "df_dim_products = df_dim_products.withColumn(\"product_key\", row_number().over(window_spec))\n",
    "\n",
    "# Reorder columns\n",
    "cols = [\"product_key\"] + [col for col in df_dim_products.columns if col != \"product_key\"]\n",
    "df_dim_products = df_dim_products.select(cols)\n",
    "\n",
    "# Preview\n",
    "df_dim_products.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e24d3a-7dfd-45d6-ae1e-a5023ed4e420",
   "metadata": {},
   "source": [
    "##### 3.2.3. Save as the <span style=\"color:darkred\">dim_products</span> table in the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b76683fe-db95-4ab7-934e-e3d28bc4e717",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_products.write.saveAsTable(f\"{dest_database}.dim_products\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6942bdac-df24-4e80-bd1b-e1dc276d2c34",
   "metadata": {},
   "source": [
    "##### 3.2.4. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7a723db4-148f-4a75-a40c-dac99aab3559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------+-------+\n",
      "|col_name                    |data_type                   |comment|\n",
      "+----------------------------+----------------------------+-------+\n",
      "|product_key                 |int                         |NULL   |\n",
      "|product_id                  |bigint                      |NULL   |\n",
      "|language_id                 |bigint                      |NULL   |\n",
      "|last_update                 |bigint                      |NULL   |\n",
      "|length                      |bigint                      |NULL   |\n",
      "|original_language_id        |string                      |NULL   |\n",
      "|rating                      |string                      |NULL   |\n",
      "|release_year                |bigint                      |NULL   |\n",
      "|rental_duration             |bigint                      |NULL   |\n",
      "|rental_rate                 |double                      |NULL   |\n",
      "|replacement_cost            |double                      |NULL   |\n",
      "|special_features            |string                      |NULL   |\n",
      "|title                       |string                      |NULL   |\n",
      "|                            |                            |       |\n",
      "|# Detailed Table Information|                            |       |\n",
      "|Catalog                     |spark_catalog               |       |\n",
      "|Database                    |rental_dlh                  |       |\n",
      "|Table                       |dim_products                |       |\n",
      "|Created Time                |Mon May 26 17:16:55 EDT 2025|       |\n",
      "|Last Access                 |UNKNOWN                     |       |\n",
      "+----------------------------+----------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+----------+-----------+-------------+------+--------------------+------+------------+---------------+-----------+----------------+--------------------------------+----------------+\n",
      "|product_key|product_id|language_id|last_update  |length|original_language_id|rating|release_year|rental_duration|rental_rate|replacement_cost|special_features                |title           |\n",
      "+-----------+----------+-----------+-------------+------+--------------------+------+------------+---------------+-----------+----------------+--------------------------------+----------------+\n",
      "|1          |1         |1          |1139979822000|86    |NULL                |PG    |2006        |6              |0.99       |20.99           |Deleted Scenes,Behind the Scenes|ACADEMY DINOSAUR|\n",
      "|2          |2         |1          |1139979822000|48    |NULL                |G     |2006        |3              |4.99       |12.99           |Trailers,Deleted Scenes         |ACE GOLDFINGER  |\n",
      "|3          |3         |1          |1139979822000|50    |NULL                |NC-17 |2006        |7              |2.99       |18.99           |Trailers,Deleted Scenes         |ADAPTATION HOLES|\n",
      "|4          |4         |1          |1139979822000|117   |NULL                |G     |2006        |5              |2.99       |26.99           |Commentaries,Behind the Scenes  |AFFAIR PREJUDICE|\n",
      "|5          |5         |1          |1139979822000|130   |NULL                |G     |2006        |6              |2.99       |22.99           |Deleted Scenes                  |AFRICAN EGG     |\n",
      "+-----------+----------+-----------+-------------+------+--------------------+------+------------+---------------+-----------+----------------+--------------------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_products\").show(truncate=False)\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_products LIMIT 5\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375bf727-5f3e-4589-a4da-d57446a376f3",
   "metadata": {},
   "source": [
    "### 4.0. Verify Dimension Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "29795432-f447-4c0b-a5a1-cc7d4b4fb008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>namespace</th>\n",
       "      <th>tableName</th>\n",
       "      <th>isTemporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rental_dlh</td>\n",
       "      <td>dim_customers</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rental_dlh</td>\n",
       "      <td>dim_date</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rental_dlh</td>\n",
       "      <td>dim_products</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rental_dlh</td>\n",
       "      <td>dim_stores</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>customers</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>stores</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    namespace      tableName  isTemporary\n",
       "0  rental_dlh  dim_customers        False\n",
       "1  rental_dlh       dim_date        False\n",
       "2  rental_dlh   dim_products        False\n",
       "3  rental_dlh     dim_stores        False\n",
       "4                  customers         True\n",
       "5                     stores         True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"USE {dest_database};\")\n",
    "spark.sql(\"SHOW TABLES\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f158f3-15d4-4b77-8080-c3d1bc68986a",
   "metadata": {},
   "source": [
    "## Section III: Integrate Reference Data with Real-Time Data\n",
    "### 6.0. Use PySpark Structured Streaming to Process (Hot Path) <span style=\"color:darkred\">Orders</span> Fact Data  \n",
    "#### 6.1. Verify the location of the source data files on the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2b506c14-a897-4ac8-a01e-590ddc2e49b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>modification_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>northwind_orders_01.json</td>\n",
       "      <td>9609</td>\n",
       "      <td>2025-03-26 22:08:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>northwind_orders_02.json</td>\n",
       "      <td>9103</td>\n",
       "      <td>2025-03-26 22:08:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>northwind_orders_03.json</td>\n",
       "      <td>9008</td>\n",
       "      <td>2025-03-26 22:08:45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name  size   modification_time\n",
       "0  northwind_orders_01.json  9609 2025-03-26 22:08:45\n",
       "1  northwind_orders_02.json  9103 2025-03-26 22:08:45\n",
       "2  northwind_orders_03.json  9008 2025-03-26 22:08:45"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file_info(orders_stream_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725f2bda-d78e-4731-9d4e-10d75d3607ba",
   "metadata": {},
   "source": [
    "#### 6.2. Create the Bronze Layer: Stage <span style=\"color:darkred\">Orders Fact table</span> Data\n",
    "##### 6.2.1. Read \"Raw\" JSON file data into a Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5e85f241-1ed8-4ff4-851d-9573f292f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, DoubleType\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),  # or DateType() if formatted correctly\n",
    "    StructField(\"ship_date\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"total\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fe375157-dfad-4442-8563-0a05cacecb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean old outputs (recommended before starting)\n",
    "remove_directory_tree(orders_output_bronze)\n",
    "remove_directory_tree(os.path.join(orders_output_bronze, '_checkpoint'))\n",
    "\n",
    "# Load the streaming orders data (Bronze layer)\n",
    "df_orders_bronze = (\n",
    "    spark.readStream\n",
    "         .format(\"json\")\n",
    "         .schema(orders_schema)  \n",
    "         .option(\"maxFilesPerTrigger\", 1)\n",
    "         .option(\"multiLine\", \"true\")\n",
    "         .load(orders_stream_dir)\n",
    ")\n",
    "df_orders_bronze.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4f5b066a-9f15-4adb-a4c7-586108308595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Directory '/Users/soniasadani/Documents/04-PySpark/spark-warehouse/rental_dlh.db/fact_orders/silver/_checkpoint' does not exist.\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_directory_tree(orders_output_bronze)\n",
    "remove_directory_tree(os.path.join(orders_output_bronze, '_checkpoint'))\n",
    "remove_directory_tree(orders_output_silver)\n",
    "remove_directory_tree(os.path.join(orders_output_silver, '_checkpoint'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e94035a-3bc4-4ba2-abf5-63ca89b4e0f7",
   "metadata": {},
   "source": [
    "##### 6.2.2. Write the Streaming Data to a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "086797ea-c9bb-409d-86f9-69ad7a3636f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['northwind_orders_02.json',\n",
       " 'northwind_orders_03.json',\n",
       " 'northwind_orders_01.json']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Show what's in your retail-org directory\n",
    "os.listdir(\"/Users/soniasadani/Documents/04-PySpark/lab_data/retail-org/streaming/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b36f8990-44cc-4f9c-bf3f-709f8a8bc890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"ship_date\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"total\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Clean any existing outputs\n",
    "remove_directory_tree(orders_output_bronze)\n",
    "remove_directory_tree(os.path.join(orders_output_bronze, '_checkpoint'))\n",
    "\n",
    "# Bronze streaming read\n",
    "df_orders_bronze = (\n",
    "    spark.readStream\n",
    "         .format(\"json\")\n",
    "         .schema(orders_schema)\n",
    "         .option(\"maxFilesPerTrigger\", 1)\n",
    "         .option(\"multiLine\", \"true\")\n",
    "         .load(orders_stream_dir)\n",
    "         .withColumn(\"receipt_time\", current_timestamp())\n",
    "         .withColumn(\"source_file\", input_file_name())\n",
    ")\n",
    "\n",
    "# Bronze streaming write\n",
    "orders_bronze_query = (\n",
    "    df_orders_bronze.writeStream\n",
    "        .format(\"parquet\")\n",
    "        .outputMode(\"append\")\n",
    "        .queryName(\"orders_bronze\")\n",
    "        .trigger(availableNow=True)\n",
    "        .option(\"checkpointLocation\", os.path.join(orders_output_bronze, '_checkpoint'))\n",
    "        .option(\"compression\", \"snappy\")\n",
    "        .start(orders_output_bronze)\n",
    ")\n",
    "\n",
    "orders_bronze_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "92120557-b80a-4e68-b035-c18fc0839c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------------+---------+------+-----+-----------------------+------------------------------------------------------------------------------------------------------------+\n",
      "|order_id|customer_id|order_date         |ship_date|status|total|receipt_time           |source_file                                                                                                 |\n",
      "+--------+-----------+-------------------+---------+------+-----+-----------------------+------------------------------------------------------------------------------------------------------------+\n",
      "|30      |27         |2006-01-15 00:00:00|NULL     |NULL  |NULL |2025-05-26 17:16:58.196|file:///Users/soniasadani/Documents/04-PySpark/lab_data/retail-org/streaming/orders/northwind_orders_01.json|\n",
      "|30      |27         |2006-01-15 00:00:00|NULL     |NULL  |NULL |2025-05-26 17:16:58.196|file:///Users/soniasadani/Documents/04-PySpark/lab_data/retail-org/streaming/orders/northwind_orders_01.json|\n",
      "|31      |4          |2006-01-20 00:00:00|NULL     |NULL  |NULL |2025-05-26 17:16:58.196|file:///Users/soniasadani/Documents/04-PySpark/lab_data/retail-org/streaming/orders/northwind_orders_01.json|\n",
      "|31      |4          |2006-01-20 00:00:00|NULL     |NULL  |NULL |2025-05-26 17:16:58.196|file:///Users/soniasadani/Documents/04-PySpark/lab_data/retail-org/streaming/orders/northwind_orders_01.json|\n",
      "|31      |4          |2006-01-20 00:00:00|NULL     |NULL  |NULL |2025-05-26 17:16:58.196|file:///Users/soniasadani/Documents/04-PySpark/lab_data/retail-org/streaming/orders/northwind_orders_01.json|\n",
      "+--------+-----------+-------------------+---------+------+-----+-----------------------+------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders_bronze_staged = spark.read.parquet(orders_output_bronze)\n",
    "df_orders_bronze_staged.show(5, truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45b461d-1d9c-4233-aae9-cae37539c0ba",
   "metadata": {},
   "source": [
    "##### 6.2.3. Unit Test: Implement Query Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "299479ba-fb32-4260-8585-14f9c47b1b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 8cca1f39-919b-4825-923e-79202c2c7a7a\n",
      "Query Name: orders_bronze\n",
      "Query Status: {'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query ID: {orders_bronze_query.id}\")\n",
    "print(f\"Query Name: {orders_bronze_query.name}\")\n",
    "print(f\"Query Status: {orders_bronze_query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bc62d72c-f857-43c0-8c37-716ef3501064",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_bronze_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a568fe6b-169a-484a-bde0-b46ab4aee66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>modification_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>northwind_orders_01.json</td>\n",
       "      <td>9609</td>\n",
       "      <td>2025-03-26 22:08:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>northwind_orders_02.json</td>\n",
       "      <td>9103</td>\n",
       "      <td>2025-03-26 22:08:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>northwind_orders_03.json</td>\n",
       "      <td>9008</td>\n",
       "      <td>2025-03-26 22:08:45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name  size   modification_time\n",
       "0  northwind_orders_01.json  9609 2025-03-26 22:08:45\n",
       "1  northwind_orders_02.json  9103 2025-03-26 22:08:45\n",
       "2  northwind_orders_03.json  9008 2025-03-26 22:08:45"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file_info(orders_stream_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e4dd9c99-cc5e-4451-a599-061fcba5e5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-----------+-------------------+---------------+--------------------+--------+------------+-------------------+------------+----------+--------+-------------------+----------+------------+--------+-----+----------+\n",
      "|customer_id|discount|employee_id|order_date         |order_detail_id|order_details_status|order_id|order_status|paid_date          |payment_type|product_id|quantity|shipped_date       |shipper_id|shipping_fee|tax_rate|taxes|unit_price|\n",
      "+-----------+--------+-----------+-------------------+---------------+--------------------+--------+------------+-------------------+------------+----------+--------+-------------------+----------+------------+--------+-----+----------+\n",
      "|27         |0       |9          |2006-01-15 00:00:00|27             |Invoiced            |30      |Closed      |2006-01-15 00:00:00|Check       |34        |100.0   |2006-01-22 00:00:00|2         |200.0       |0       |0.0  |14.0      |\n",
      "|27         |0       |9          |2006-01-15 00:00:00|28             |Invoiced            |30      |Closed      |2006-01-15 00:00:00|Check       |80        |30.0    |2006-01-22 00:00:00|2         |200.0       |0       |0.0  |3.5       |\n",
      "|4          |0       |3          |2006-01-20 00:00:00|29             |Invoiced            |31      |Closed      |2006-01-20 00:00:00|Credit Card |7         |10.0    |2006-01-22 00:00:00|1         |5.0         |0       |0.0  |30.0      |\n",
      "|4          |0       |3          |2006-01-20 00:00:00|30             |Invoiced            |31      |Closed      |2006-01-20 00:00:00|Credit Card |51        |10.0    |2006-01-22 00:00:00|1         |5.0         |0       |0.0  |53.0      |\n",
      "|4          |0       |3          |2006-01-20 00:00:00|31             |Invoiced            |31      |Closed      |2006-01-20 00:00:00|Credit Card |80        |10.0    |2006-01-22 00:00:00|1         |5.0         |0       |0.0  |3.5       |\n",
      "|12         |0       |4          |2006-01-22 00:00:00|32             |Invoiced            |32      |Closed      |2006-01-22 00:00:00|Credit Card |1         |15.0    |2006-01-22 00:00:00|2         |5.0         |0       |0.0  |18.0      |\n",
      "|12         |0       |4          |2006-01-22 00:00:00|33             |Invoiced            |32      |Closed      |2006-01-22 00:00:00|Credit Card |43        |20.0    |2006-01-22 00:00:00|2         |5.0         |0       |0.0  |46.0      |\n",
      "|8          |0       |6          |2006-01-30 00:00:00|34             |Invoiced            |33      |Closed      |2006-01-30 00:00:00|Credit Card |19        |30.0    |2006-01-31 00:00:00|3         |50.0        |0       |0.0  |9.2       |\n",
      "|4          |0       |9          |2006-02-06 00:00:00|35             |Invoiced            |34      |Closed      |2006-02-06 00:00:00|Check       |19        |20.0    |2006-02-07 00:00:00|3         |4.0         |0       |0.0  |9.2       |\n",
      "|29         |0       |3          |2006-02-10 00:00:00|36             |Invoiced            |35      |Closed      |2006-02-10 00:00:00|Check       |48        |10.0    |2006-02-12 00:00:00|2         |7.0         |0       |0.0  |12.75     |\n",
      "|3          |0       |4          |2006-02-23 00:00:00|37             |Invoiced            |36      |Closed      |2006-02-23 00:00:00|Cash        |41        |200.0   |2006-02-25 00:00:00|2         |7.0         |0       |0.0  |9.65      |\n",
      "|6          |0       |8          |2006-03-06 00:00:00|38             |Invoiced            |37      |Closed      |2006-03-06 00:00:00|Credit Card |8         |17.0    |2006-03-09 00:00:00|2         |12.0        |0       |0.0  |40.0      |\n",
      "|28         |0       |9          |2006-03-10 00:00:00|39             |Invoiced            |38      |Closed      |2006-03-10 00:00:00|Check       |43        |300.0   |2006-03-11 00:00:00|3         |10.0        |0       |0.0  |46.0      |\n",
      "|8          |0       |3          |2006-03-22 00:00:00|40             |Invoiced            |39      |Closed      |2006-03-22 00:00:00|Check       |48        |100.0   |2006-03-24 00:00:00|3         |5.0         |0       |0.0  |12.75     |\n",
      "|10         |0       |4          |2006-03-24 00:00:00|41             |Invoiced            |40      |Closed      |2006-03-24 00:00:00|Credit Card |81        |200.0   |2006-03-24 00:00:00|2         |9.0         |0       |0.0  |2.99      |\n",
      "|7          |0       |1          |2006-03-24 00:00:00|42             |Allocated           |41      |New         |NULL               |NULL        |43        |300.0   |NULL               |NULL      |0.0         |0       |0.0  |46.0      |\n",
      "|10         |0       |1          |2006-03-24 00:00:00|43             |Invoiced            |42      |Shipped     |NULL               |NULL        |6         |10.0    |2006-04-07 00:00:00|1         |0.0         |0       |0.0  |25.0      |\n",
      "|10         |0       |1          |2006-03-24 00:00:00|44             |Invoiced            |42      |Shipped     |NULL               |NULL        |4         |10.0    |2006-04-07 00:00:00|1         |0.0         |0       |0.0  |22.0      |\n",
      "|10         |0       |1          |2006-03-24 00:00:00|45             |Invoiced            |42      |Shipped     |NULL               |NULL        |19        |10.0    |2006-04-07 00:00:00|1         |0.0         |0       |0.0  |9.2       |\n",
      "|11         |0       |1          |2006-03-24 00:00:00|46             |Allocated           |43      |New         |NULL               |NULL        |80        |20.0    |NULL               |3         |0.0         |0       |0.0  |3.5       |\n",
      "+-----------+--------+-----------+-------------------+---------------+--------------------+--------+------------+-------------------+------------+----------+--------+-------------------+----------+------------+--------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option(\"multiLine\", \"true\").json(orders_stream_dir).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d2c21b23-f007-4eae-ac3b-4830ecf49554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------------+---------+------+-----+-----------------------+------------------------------------------------------------------------------------------------------------+\n",
      "|order_id|customer_id|order_date         |ship_date|status|total|receipt_time           |source_file                                                                                                 |\n",
      "+--------+-----------+-------------------+---------+------+-----+-----------------------+------------------------------------------------------------------------------------------------------------+\n",
      "|30      |27         |2006-01-15 00:00:00|NULL     |NULL  |NULL |2025-05-26 17:16:58.196|file:///Users/soniasadani/Documents/04-PySpark/lab_data/retail-org/streaming/orders/northwind_orders_01.json|\n",
      "|30      |27         |2006-01-15 00:00:00|NULL     |NULL  |NULL |2025-05-26 17:16:58.196|file:///Users/soniasadani/Documents/04-PySpark/lab_data/retail-org/streaming/orders/northwind_orders_01.json|\n",
      "|31      |4          |2006-01-20 00:00:00|NULL     |NULL  |NULL |2025-05-26 17:16:58.196|file:///Users/soniasadani/Documents/04-PySpark/lab_data/retail-org/streaming/orders/northwind_orders_01.json|\n",
      "|31      |4          |2006-01-20 00:00:00|NULL     |NULL  |NULL |2025-05-26 17:16:58.196|file:///Users/soniasadani/Documents/04-PySpark/lab_data/retail-org/streaming/orders/northwind_orders_01.json|\n",
      "|31      |4          |2006-01-20 00:00:00|NULL     |NULL  |NULL |2025-05-26 17:16:58.196|file:///Users/soniasadani/Documents/04-PySpark/lab_data/retail-org/streaming/orders/northwind_orders_01.json|\n",
      "+--------+-----------+-------------------+---------+------+-----+-----------------------+------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(orders_output_bronze).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6c9ea630-430a-4b8e-9313-bf51b2c31a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n",
      "|order_date         |customer_id|\n",
      "+-------------------+-----------+\n",
      "|2006-01-22 00:00:00|12         |\n",
      "|2006-06-05 00:00:00|29         |\n",
      "|2006-04-05 00:00:00|9          |\n",
      "|2006-03-06 00:00:00|6          |\n",
      "|2006-03-24 00:00:00|1          |\n",
      "|2006-04-30 00:00:00|8          |\n",
      "|2006-06-07 00:00:00|28         |\n",
      "|2006-06-08 00:00:00|6          |\n",
      "|2006-02-06 00:00:00|4          |\n",
      "|2006-04-07 00:00:00|28         |\n",
      "|2006-03-24 00:00:00|11         |\n",
      "|2006-04-05 00:00:00|25         |\n",
      "|2006-04-05 00:00:00|8          |\n",
      "|2006-04-08 00:00:00|6          |\n",
      "|2006-03-24 00:00:00|7          |\n",
      "|2006-04-03 00:00:00|6          |\n",
      "|2006-04-05 00:00:00|26         |\n",
      "|2006-03-10 00:00:00|28         |\n",
      "|2006-03-22 00:00:00|8          |\n",
      "|2006-01-20 00:00:00|4          |\n",
      "+-------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(orders_output_bronze).select(\"order_date\", \"customer_id\").distinct().show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d482be62-2e8f-4f64-b27f-c1b7a31ce1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|customer_id|\n",
      "+-----------+\n",
      "|23200537   |\n",
      "|31701287   |\n",
      "|61992418   |\n",
      "|16243030   |\n",
      "|17185921   |\n",
      "|14362584   |\n",
      "|15424531   |\n",
      "|14397430   |\n",
      "|15271602   |\n",
      "|25765433   |\n",
      "|16903400   |\n",
      "|4101697    |\n",
      "|21442856   |\n",
      "|3609418    |\n",
      "|19035609   |\n",
      "|12866703   |\n",
      "|19506364   |\n",
      "|16605520   |\n",
      "|5404337    |\n",
      "|26801418   |\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+--------+\n",
      "|full_date |date_key|\n",
      "+----------+--------+\n",
      "|2002-08-27|20020827|\n",
      "|2004-01-19|20040119|\n",
      "|2004-02-18|20040218|\n",
      "|2005-04-16|20050416|\n",
      "|2005-09-02|20050902|\n",
      "|2005-10-30|20051030|\n",
      "|2006-03-01|20060301|\n",
      "|2006-08-09|20060809|\n",
      "|2006-12-15|20061215|\n",
      "|2007-04-11|20070411|\n",
      "|2007-09-21|20070921|\n",
      "|2008-06-05|20080605|\n",
      "|2008-11-29|20081129|\n",
      "|2009-01-04|20090104|\n",
      "|2009-01-06|20090106|\n",
      "|2009-02-09|20090209|\n",
      "|2009-12-18|20091218|\n",
      "|2010-05-07|20100507|\n",
      "|2010-07-04|20100704|\n",
      "|2010-08-23|20100823|\n",
      "+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dim_customers.select(\"customer_id\").distinct().show(20, truncate=False)\n",
    "df_dim_date.select(\"full_date\", \"date_key\").distinct().show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8241a9b4-2592-41b2-841f-53b29185a12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Reload Bronze layer\n",
    "df_orders_bronze = spark.read.parquet(orders_output_bronze)\n",
    "\n",
    "# Extract distinct customer IDs\n",
    "df_dim_customers = df_orders_bronze.select(\"customer_id\").distinct()\n",
    "\n",
    "# Add surrogate key\n",
    "window_spec = Window.orderBy(\"customer_id\")\n",
    "df_dim_customers = df_dim_customers.withColumn(\"customer_key\", row_number().over(window_spec))\n",
    "\n",
    "# Reorder columns\n",
    "df_dim_customers = df_dim_customers.select(\"customer_key\", \"customer_id\")\n",
    "\n",
    "# Save updated dimension\n",
    "df_dim_customers.write.saveAsTable(f\"{dest_database}.dim_customers\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "761bc470-fdd3-41e8-a72f-86ae55d2c6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract distinct order dates\n",
    "df_dim_date = df_orders_bronze.selectExpr(\"CAST(order_date AS DATE) AS full_date\").distinct()\n",
    "\n",
    "# Add surrogate key\n",
    "window_spec = Window.orderBy(\"full_date\")\n",
    "df_dim_date = df_dim_date.withColumn(\"date_key\", row_number().over(window_spec))\n",
    "\n",
    "# Reorder\n",
    "df_dim_date = df_dim_date.select(\"date_key\", \"full_date\")\n",
    "\n",
    "# Save updated dimension\n",
    "df_dim_date.write.saveAsTable(f\"{dest_database}.dim_date\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1baf025-d6fd-4b69-89d2-dd89c3205496",
   "metadata": {},
   "source": [
    "#### 6.3. Create the Silver Layer: Integrate \"Cold-path\" Data & Make Transformations\n",
    "##### 6.3.1. Prepare Role-Playing Dimension Primary and Business Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "28b1f33c-0ebd-4156-9f14-e8588cab2c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create role-playing versions of the dim_date table\n",
    "df_dim_order_date = df_dim_date.select(col(\"date_key\").alias(\"order_date_key\"), col(\"full_date\").alias(\"order_full_date\"))\n",
    "df_dim_paid_date = df_dim_date.select(col(\"date_key\").alias(\"paid_date_key\"), col(\"full_date\").alias(\"paid_full_date\"))\n",
    "df_dim_shipped_date = df_dim_date.select(col(\"date_key\").alias(\"shipped_date_key\"), col(\"full_date\").alias(\"shipped_full_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7b0437-66a7-4386-9084-4594faab6df0",
   "metadata": {},
   "source": [
    "##### 6.3.2. Define Silver Query to Join Streaming with Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d3b1895d-ebd5-4e7f-9056-3fab7338c0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import LongType, DateType\n",
    "\n",
    "df_dim_order_date = df_dim_date.select(\n",
    "    col(\"date_key\").alias(\"order_date_key\"),\n",
    "    col(\"full_date\").alias(\"order_full_date\")\n",
    ")\n",
    "\n",
    "df_dim_shipped_date = df_dim_date.select(\n",
    "    col(\"date_key\").alias(\"shipped_date_key\"),\n",
    "    col(\"full_date\").alias(\"shipped_full_date\")\n",
    ")\n",
    "\n",
    "df_orders_silver = (\n",
    "    spark.readStream\n",
    "        .format(\"parquet\")\n",
    "        .load(orders_output_bronze)\n",
    "        .join(df_dim_customers.select(\"customer_id\", \"customer_key\"), on=\"customer_id\", how=\"inner\")\n",
    "        .join(df_dim_order_date, col(\"order_date\").cast(DateType()) == col(\"order_full_date\").cast(DateType()), \"inner\")\n",
    "        .join(df_dim_shipped_date, col(\"ship_date\").cast(DateType()) == col(\"shipped_full_date\").cast(DateType()), \"left_outer\")\n",
    "        .select(\n",
    "            col(\"order_id\").cast(LongType()),\n",
    "            col(\"customer_key\").cast(LongType()),\n",
    "            col(\"order_date_key\").cast(LongType()),\n",
    "            col(\"shipped_date_key\").cast(LongType()),\n",
    "            col(\"status\"),\n",
    "            col(\"total\"),\n",
    "            col(\"receipt_time\"),\n",
    "            col(\"source_file\")\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cce36782-37f7-4a7a-baf8-bc6c9d7e647d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orders_silver.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "adcb25db-964b-41ed-bfc8-9ca633c219b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- customer_key: long (nullable = false)\n",
      " |-- order_date_key: long (nullable = false)\n",
      " |-- shipped_date_key: long (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- total: double (nullable = true)\n",
      " |-- receipt_time: timestamp (nullable = true)\n",
      " |-- source_file: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders_silver.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6c9fd4-c595-4c38-adb3-c5a3a1f13f53",
   "metadata": {},
   "source": [
    "##### 6.3.3. Write the Transformed Streaming data to the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "145b60d0-0985-4641-822a-7e7995ce0785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define checkpoint directory for Silver output\n",
    "orders_checkpoint_silver = os.path.join(orders_output_silver, '_checkpoint')\n",
    "\n",
    "# Write the Silver stream to Parquet files\n",
    "orders_silver_query = (\n",
    "    df_orders_silver.writeStream\n",
    "        .format(\"parquet\")\n",
    "        .outputMode(\"append\")\n",
    "        .queryName(\"orders_silver\")\n",
    "        .trigger(availableNow=True)\n",
    "        .option(\"checkpointLocation\", orders_checkpoint_silver)\n",
    "        .option(\"compression\", \"snappy\")\n",
    "        .start(orders_output_silver)\n",
    ")\n",
    "\n",
    "# Wait until all files in the stream are processed\n",
    "orders_silver_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bf04c9c0-d8d7-40a2-823d-110ac9adc1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- customer_key: long (nullable = false)\n",
      " |-- order_date_key: long (nullable = false)\n",
      " |-- shipped_date_key: long (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- total: double (nullable = true)\n",
      " |-- receipt_time: timestamp (nullable = true)\n",
      " |-- source_file: string (nullable = true)\n",
      "\n",
      "+--------+------------+--------------+\n",
      "|order_id|customer_key|order_date_key|\n",
      "+--------+------------+--------------+\n",
      "|30      |13          |1             |\n",
      "|30      |13          |1             |\n",
      "|31      |3           |2             |\n",
      "|31      |3           |2             |\n",
      "|31      |3           |2             |\n",
      "|32      |10          |3             |\n",
      "|32      |10          |3             |\n",
      "|33      |6           |4             |\n",
      "|34      |3           |5             |\n",
      "|35      |15          |6             |\n",
      "+--------+------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_silver_check = spark.read.parquet(orders_output_silver)\n",
    "\n",
    "df_silver_check.printSchema()\n",
    "\n",
    "df_silver_check.select(\"order_id\", \"customer_key\", \"order_date_key\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "65c109e8-a1c6-4bef-a075-98242ae127ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------------+----------------+------+-----+--------------------+--------------------+\n",
      "|order_id|customer_key|order_date_key|shipped_date_key|status|total|        receipt_time|         source_file|\n",
      "+--------+------------+--------------+----------------+------+-----+--------------------+--------------------+\n",
      "|      30|          13|             1|            NULL|  NULL| NULL|2025-05-26 17:16:...|file:///Users/son...|\n",
      "|      30|          13|             1|            NULL|  NULL| NULL|2025-05-26 17:16:...|file:///Users/son...|\n",
      "|      31|           3|             2|            NULL|  NULL| NULL|2025-05-26 17:16:...|file:///Users/son...|\n",
      "|      31|           3|             2|            NULL|  NULL| NULL|2025-05-26 17:16:...|file:///Users/son...|\n",
      "|      31|           3|             2|            NULL|  NULL| NULL|2025-05-26 17:16:...|file:///Users/son...|\n",
      "+--------+------------+--------------+----------------+------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+------------+--------------+----------------+------+-----+-----------------------+------------------------------------------------------------------------------------------------------------+\n",
      "|order_id|customer_key|order_date_key|shipped_date_key|status|total|receipt_time           |source_file                                                                                                 |\n",
      "+--------+------------+--------------+----------------+------+-----+-----------------------+------------------------------------------------------------------------------------------------------------+\n",
      "|30      |13          |1             |NULL            |NULL  |NULL |2025-05-26 17:16:58.196|file:///Users/soniasadani/Documents/04-PySpark/lab_data/retail-org/streaming/orders/northwind_orders_01.json|\n",
      "|30      |13          |1             |NULL            |NULL  |NULL |2025-05-26 17:16:58.196|file:///Users/soniasadani/Documents/04-PySpark/lab_data/retail-org/streaming/orders/northwind_orders_01.json|\n",
      "|31      |3           |2             |NULL            |NULL  |NULL |2025-05-26 17:16:58.196|file:///Users/soniasadani/Documents/04-PySpark/lab_data/retail-org/streaming/orders/northwind_orders_01.json|\n",
      "|31      |3           |2             |NULL            |NULL  |NULL |2025-05-26 17:16:58.196|file:///Users/soniasadani/Documents/04-PySpark/lab_data/retail-org/streaming/orders/northwind_orders_01.json|\n",
      "|31      |3           |2             |NULL            |NULL  |NULL |2025-05-26 17:16:58.196|file:///Users/soniasadani/Documents/04-PySpark/lab_data/retail-org/streaming/orders/northwind_orders_01.json|\n",
      "+--------+------------+--------------+----------------+------+-----+-----------------------+------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(orders_output_silver).show(5)\n",
    "spark.read.parquet(orders_output_silver).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "84d96985-a8d4-44cb-b30f-b96fa58884b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+------------+\n",
      "|order_id|order_date_key|customer_key|\n",
      "+--------+--------------+------------+\n",
      "|      30|             1|          13|\n",
      "|      30|             1|          13|\n",
      "|      31|             2|           3|\n",
      "|      31|             2|           3|\n",
      "|      31|             2|           3|\n",
      "|      32|             3|          10|\n",
      "|      32|             3|          10|\n",
      "|      33|             4|           6|\n",
      "|      34|             5|           3|\n",
      "|      35|             6|          15|\n",
      "|      36|             7|           2|\n",
      "|      37|             8|           4|\n",
      "|      38|             9|          14|\n",
      "|      39|            10|           6|\n",
      "|      40|            11|           8|\n",
      "|      41|            11|           5|\n",
      "|      42|            11|           8|\n",
      "|      42|            11|           8|\n",
      "|      42|            11|           8|\n",
      "|      43|            11|           9|\n",
      "+--------+--------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(orders_output_silver).select(\"order_id\", \"order_date_key\", \"customer_key\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a6c445-16dd-4752-9e46-6ba7e9cef121",
   "metadata": {},
   "source": [
    "##### 6.3.4. Unit Test: Implement Query Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ea1e7be2-fea6-4578-9237-344983c52111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 18d303ec-f6ad-451e-a5c9-525d5cb23541\n",
      "Query Name: orders_silver\n",
      "Query Status: {'message': 'Stopped', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query ID: {orders_silver_query.id}\")\n",
    "print(f\"Query Name: {orders_silver_query.name}\")\n",
    "print(f\"Query Status: {orders_silver_query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1493145c-19b3-4446-84e1-b9a37ecf2010",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_silver_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "06fb0550-94c1-40e3-a559-74ddf9406483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------------+----------------+------+-----+-----------------------+------------------------------------------------------------------------------------------------------------+\n",
      "|order_id|customer_key|order_date_key|shipped_date_key|status|total|receipt_time           |source_file                                                                                                 |\n",
      "+--------+------------+--------------+----------------+------+-----+-----------------------+------------------------------------------------------------------------------------------------------------+\n",
      "|30      |13          |1             |NULL            |NULL  |NULL |2025-05-26 17:16:58.196|file:///Users/soniasadani/Documents/04-PySpark/lab_data/retail-org/streaming/orders/northwind_orders_01.json|\n",
      "|30      |13          |1             |NULL            |NULL  |NULL |2025-05-26 17:16:58.196|file:///Users/soniasadani/Documents/04-PySpark/lab_data/retail-org/streaming/orders/northwind_orders_01.json|\n",
      "|31      |3           |2             |NULL            |NULL  |NULL |2025-05-26 17:16:58.196|file:///Users/soniasadani/Documents/04-PySpark/lab_data/retail-org/streaming/orders/northwind_orders_01.json|\n",
      "|31      |3           |2             |NULL            |NULL  |NULL |2025-05-26 17:16:58.196|file:///Users/soniasadani/Documents/04-PySpark/lab_data/retail-org/streaming/orders/northwind_orders_01.json|\n",
      "|31      |3           |2             |NULL            |NULL  |NULL |2025-05-26 17:16:58.196|file:///Users/soniasadani/Documents/04-PySpark/lab_data/retail-org/streaming/orders/northwind_orders_01.json|\n",
      "+--------+------------+--------------+----------------+------+-----+-----------------------+------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(orders_output_silver).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e4dfc6b9-2c12-4d4c-a14c-a009fd75e943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(orders_output_silver).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e22b82bf-9244-4f6c-a566-204dce825e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(orders_output_bronze).select(\"order_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e51f8976-976e-4e27-bea8-cc57ea92ed33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- customer_key: long (nullable = false)\n",
      " |-- order_date_key: long (nullable = false)\n",
      " |-- shipped_date_key: long (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- total: double (nullable = true)\n",
      " |-- receipt_time: timestamp (nullable = true)\n",
      " |-- source_file: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_silver_check.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "323856e1-16e0-4f46-8ab9-8d17c51068e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------------+\n",
      "|order_id|customer_key|order_date_key|\n",
      "+--------+------------+--------------+\n",
      "|30      |13          |1             |\n",
      "|30      |13          |1             |\n",
      "|31      |3           |2             |\n",
      "|31      |3           |2             |\n",
      "|31      |3           |2             |\n",
      "|32      |10          |3             |\n",
      "|32      |10          |3             |\n",
      "|33      |6           |4             |\n",
      "|34      |3           |5             |\n",
      "|35      |15          |6             |\n",
      "+--------+------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_silver_check.select(\"order_id\", \"customer_key\", \"order_date_key\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "01230476-b036-4187-831f-fe5e4e19a74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- customer_key: long (nullable = false)\n",
      " |-- order_date_key: long (nullable = false)\n",
      " |-- shipped_date_key: long (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- total: double (nullable = true)\n",
      " |-- receipt_time: timestamp (nullable = true)\n",
      " |-- source_file: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_silver_check.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f9687dcb-7ce0-49dc-b320-d4656cc890e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|order_date_key|\n",
      "+--------------+\n",
      "|             1|\n",
      "|             2|\n",
      "|             3|\n",
      "|             4|\n",
      "|             5|\n",
      "|             6|\n",
      "|             7|\n",
      "|             8|\n",
      "|             9|\n",
      "|            10|\n",
      "|            11|\n",
      "|            12|\n",
      "|            13|\n",
      "|            14|\n",
      "|            15|\n",
      "|            16|\n",
      "|            17|\n",
      "|            18|\n",
      "|            19|\n",
      "|            20|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(orders_output_silver).select(\"order_date_key\").distinct().orderBy(\"order_date_key\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "68915e20-8110-4bcd-a84c-1d02579c9e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|date_key|\n",
      "+--------+\n",
      "|       1|\n",
      "|       2|\n",
      "|       3|\n",
      "|       4|\n",
      "|       5|\n",
      "|       6|\n",
      "|       7|\n",
      "|       8|\n",
      "|       9|\n",
      "|      10|\n",
      "|      11|\n",
      "|      12|\n",
      "|      13|\n",
      "|      14|\n",
      "|      15|\n",
      "|      16|\n",
      "|      17|\n",
      "|      18|\n",
      "|      19|\n",
      "|      20|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dim_date.select(\"date_key\").distinct().orderBy(\"date_key\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "86017336-f693-4cba-9601-7cb2aac449bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|order_date_key|\n",
      "+--------------+\n",
      "|             1|\n",
      "|             2|\n",
      "|             3|\n",
      "|             4|\n",
      "|             5|\n",
      "|             6|\n",
      "|             7|\n",
      "|             8|\n",
      "|             9|\n",
      "|            10|\n",
      "|            11|\n",
      "|            12|\n",
      "|            13|\n",
      "|            14|\n",
      "|            15|\n",
      "|            16|\n",
      "|            17|\n",
      "|            18|\n",
      "|            19|\n",
      "|            20|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(orders_output_silver).select(\"order_date_key\").distinct().orderBy(\"order_date_key\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "db4cc1f9-bd20-41c0-b9ce-305073eb5265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|date_key|\n",
      "+--------+\n",
      "|       1|\n",
      "|       2|\n",
      "|       3|\n",
      "|       4|\n",
      "|       5|\n",
      "|       6|\n",
      "|       7|\n",
      "|       8|\n",
      "|       9|\n",
      "|      10|\n",
      "|      11|\n",
      "|      12|\n",
      "|      13|\n",
      "|      14|\n",
      "|      15|\n",
      "|      16|\n",
      "|      17|\n",
      "|      18|\n",
      "|      19|\n",
      "|      20|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dim_date.select(\"date_key\").distinct().orderBy(\"date_key\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "51bf7864-665a-4b7c-8acc-b098e10169c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month, date_format\n",
    "\n",
    "# Add month columns\n",
    "df_dim_date = df_dim_date.withColumn(\"month_of_year\", month(col(\"full_date\").cast(\"date\")))\n",
    "df_dim_date = df_dim_date.withColumn(\"month_name\", date_format(col(\"full_date\").cast(\"date\"), \"MMMM\"))\n",
    "\n",
    "# Overwrite the table so downstream cells have access to these columns\n",
    "df_dim_date.write.mode(\"overwrite\").saveAsTable(f\"{dest_database}.dim_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1bc5d88f-c9df-463f-aa9f-41f3d3fd059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_date = spark.read.table(f\"{dest_database}.dim_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6f1fc155-0663-4d21-a54d-631151c0b9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------------+-------------+\n",
      "|month_of_year|month_name|customer_key|orders_placed|\n",
      "+-------------+----------+------------+-------------+\n",
      "|1            |January   |3           |3            |\n",
      "|1            |January   |6           |1            |\n",
      "|1            |January   |10          |2            |\n",
      "|1            |January   |13          |2            |\n",
      "|2            |February  |2           |1            |\n",
      "|2            |February  |3           |1            |\n",
      "|2            |February  |15          |1            |\n",
      "|3            |March     |1           |3            |\n",
      "|3            |March     |4           |1            |\n",
      "|3            |March     |5           |1            |\n",
      "|3            |March     |6           |1            |\n",
      "|3            |March     |8           |4            |\n",
      "|3            |March     |9           |2            |\n",
      "|3            |March     |14          |1            |\n",
      "|4            |April     |2           |4            |\n",
      "|4            |April     |3           |3            |\n",
      "|4            |April     |4           |2            |\n",
      "|4            |April     |6           |3            |\n",
      "|4            |April     |7           |2            |\n",
      "|4            |April     |11          |1            |\n",
      "+-------------+----------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Static join test — simulate the Gold aggregation with a batch read of Silver\n",
    "df_orders_silver = spark.read.parquet(orders_output_silver)\n",
    "\n",
    "df_static_test = (\n",
    "    df_orders_silver\n",
    "        .join(\n",
    "            df_dim_date,\n",
    "            df_dim_date.date_key.cast(\"int\") == col(\"order_date_key\").cast(\"int\")\n",
    "        )\n",
    "        .groupBy(\"month_of_year\", \"month_name\", \"customer_key\")\n",
    "        .agg(count(\"*\").alias(\"orders_placed\"))\n",
    "        .orderBy(\"month_of_year\", \"customer_key\")\n",
    ")\n",
    "\n",
    "df_static_test.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "34f5fe5a-17a0-4289-8660-5c7d85ff9ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------------+----------+\n",
      "|order_id|customer_key|order_date_key|month_name|\n",
      "+--------+------------+--------------+----------+\n",
      "|30      |13          |1             |January   |\n",
      "|30      |13          |1             |January   |\n",
      "|31      |3           |2             |January   |\n",
      "|31      |3           |2             |January   |\n",
      "|31      |3           |2             |January   |\n",
      "|32      |10          |3             |January   |\n",
      "|32      |10          |3             |January   |\n",
      "|33      |6           |4             |January   |\n",
      "|34      |3           |5             |February  |\n",
      "|35      |15          |6             |February  |\n",
      "+--------+------------+--------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test_join = (\n",
    "    spark.read.parquet(orders_output_silver)\n",
    "         .join(df_dim_date, col(\"order_date_key\").cast(\"int\") == df_dim_date[\"date_key\"].cast(\"int\"))\n",
    ")\n",
    "\n",
    "df_test_join.select(\"order_id\", \"customer_key\", \"order_date_key\", \"month_name\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb44a5a-b470-4928-bc73-b5ed9ff8da74",
   "metadata": {},
   "source": [
    "#### 6.4. Create Gold Layer: Perform Aggregations\n",
    "##### 6.4.1. Define a Query to Create a Business Report\n",
    "Create a new Gold table using the PySpark API. The table should include the number of Products sold per Category each Month. The results should include The Month, Product Category and Number of Products sold, sorted by the month number when the orders were placed: e.g., January, February, March."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7ccd5a0f-668a-4ce0-81cf-5039a13331f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month, date_format\n",
    "\n",
    "df_dim_date = df_dim_date.withColumn(\"month_of_year\", month(col(\"full_date\").cast(\"date\")))\n",
    "df_dim_date = df_dim_date.withColumn(\"month_name\", date_format(col(\"full_date\").cast(\"date\"), \"MMMM\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "05c41a53-f7aa-4208-9cee-d2a45757e976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month_of_year: integer (nullable = true)\n",
      " |-- month_name: string (nullable = true)\n",
      " |-- customer_key: long (nullable = true)\n",
      " |-- orders_placed: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, asc\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df_orders_by_customer_gold = (\n",
    "    spark.readStream\n",
    "         .format(\"parquet\")\n",
    "         .load(orders_output_silver)\n",
    "         .join(\n",
    "             df_dim_date,\n",
    "             df_dim_date.date_key.cast(IntegerType()) == col(\"order_date_key\").cast(IntegerType())\n",
    "         )\n",
    "         .groupBy(\"month_of_year\", \"month_name\", \"customer_key\")\n",
    "         .agg(count(\"*\").alias(\"orders_placed\"))\n",
    "         .orderBy(asc(\"month_of_year\"), asc(\"customer_key\"))\n",
    ")\n",
    "\n",
    "df_orders_by_customer_gold.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f3ad6aad-4d96-48e9-ac08-84a866bf9766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|order_date_key|\n",
      "+--------------+\n",
      "|            19|\n",
      "|            22|\n",
      "|             7|\n",
      "|             6|\n",
      "|             9|\n",
      "|            17|\n",
      "|             5|\n",
      "|             1|\n",
      "|            10|\n",
      "|             3|\n",
      "+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+\n",
      "|date_key|\n",
      "+--------+\n",
      "|       1|\n",
      "|       2|\n",
      "|       3|\n",
      "|       4|\n",
      "|       5|\n",
      "|       6|\n",
      "|       7|\n",
      "|       8|\n",
      "|       9|\n",
      "|      10|\n",
      "+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check 10 example values from Silver fact\n",
    "spark.read.parquet(orders_output_silver).select(\"order_date_key\").distinct().show(10)\n",
    "\n",
    "# Check 10 values from the Date Dimension\n",
    "df_dim_date.select(\"date_key\").distinct().orderBy(\"date_key\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "ed8ac44c-ff15-4252-a37b-40c63e2ea870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------+\n",
      "|date_key|month_of_year|month_name|\n",
      "+--------+-------------+----------+\n",
      "|1       |1            |January   |\n",
      "|2       |1            |January   |\n",
      "|3       |1            |January   |\n",
      "|4       |1            |January   |\n",
      "|5       |2            |February  |\n",
      "|6       |2            |February  |\n",
      "|7       |2            |February  |\n",
      "|8       |3            |March     |\n",
      "|9       |3            |March     |\n",
      "|10      |3            |March     |\n",
      "+--------+-------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dim_date.select(\"date_key\", \"month_of_year\", \"month_name\").orderBy(\"date_key\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb8abc8-c889-4ed2-8d37-f15bb1c27ffb",
   "metadata": {},
   "source": [
    "##### 6.4.2. Write the Streaming data to a Parquet File in \"Complete\" mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d1354d07-263f-4a04-99ba-28b22a7272d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in spark.streams.active:\n",
    "    q.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b5aa3647-59f5-481c-b22f-27db88561b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, asc\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df_orders_by_customer_gold = (\n",
    "    spark.readStream\n",
    "         .format(\"parquet\")\n",
    "         .load(orders_output_silver)\n",
    "         .join(\n",
    "             df_dim_date,\n",
    "             df_dim_date.date_key.cast(IntegerType()) == col(\"order_date_key\").cast(IntegerType())\n",
    "         )\n",
    "         .groupBy(\"month_of_year\", \"month_name\", \"customer_key\")\n",
    "         .agg(count(\"*\").alias(\"orders_placed\"))\n",
    "         .orderBy(asc(\"month_of_year\"), asc(\"customer_key\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "3bf6dbcf-038c-47b4-8598-cc2d65aee062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stream has processed 3 batchs\n"
     ]
    }
   ],
   "source": [
    "orders_gold_query = (\n",
    "    df_orders_by_customer_gold.writeStream\n",
    "        .format(\"memory\")\n",
    "        .outputMode(\"complete\")\n",
    "        .queryName(\"fact_orders_by_customer_month\")\n",
    "        .start()\n",
    ")\n",
    "\n",
    "wait_until_stream_is_ready(orders_gold_query, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "1cd6d07c-2bba-4222-a67b-d9744b25c1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------------+-------------+\n",
      "|month_of_year|month_name|customer_key|orders_placed|\n",
      "+-------------+----------+------------+-------------+\n",
      "|1            |January   |3           |3            |\n",
      "|1            |January   |6           |1            |\n",
      "|1            |January   |10          |2            |\n",
      "|1            |January   |13          |2            |\n",
      "|2            |February  |2           |1            |\n",
      "|2            |February  |3           |1            |\n",
      "|2            |February  |15          |1            |\n",
      "|3            |March     |1           |3            |\n",
      "|3            |March     |4           |1            |\n",
      "|3            |March     |5           |1            |\n",
      "|3            |March     |6           |1            |\n",
      "|3            |March     |8           |4            |\n",
      "|3            |March     |9           |2            |\n",
      "|3            |March     |14          |1            |\n",
      "|4            |April     |2           |4            |\n",
      "|4            |April     |3           |3            |\n",
      "|4            |April     |4           |2            |\n",
      "|4            |April     |6           |3            |\n",
      "|4            |April     |7           |2            |\n",
      "|4            |April     |11          |1            |\n",
      "+-------------+----------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM fact_orders_by_customer_month ORDER BY month_of_year, customer_key\").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b627bd3d-d17b-4ab2-b62a-1b988d10a7c6",
   "metadata": {},
   "source": [
    "##### 6.4.3. Query the Gold Data from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "c1d4cf7a-de59-4596-a8af-0e84790d7b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month_of_year: integer (nullable = true)\n",
      " |-- month_name: string (nullable = true)\n",
      " |-- customer_key: long (nullable = true)\n",
      " |-- orders_placed: long (nullable = false)\n",
      "\n",
      "+-------------+----------+------------+-------------+\n",
      "|month_of_year|month_name|customer_key|orders_placed|\n",
      "+-------------+----------+------------+-------------+\n",
      "|1            |January   |3           |3            |\n",
      "|1            |January   |6           |1            |\n",
      "|1            |January   |10          |2            |\n",
      "|1            |January   |13          |2            |\n",
      "|2            |February  |2           |1            |\n",
      "|2            |February  |3           |1            |\n",
      "|2            |February  |15          |1            |\n",
      "|3            |March     |1           |3            |\n",
      "|3            |March     |4           |1            |\n",
      "|3            |March     |5           |1            |\n",
      "|3            |March     |6           |1            |\n",
      "|3            |March     |8           |4            |\n",
      "|3            |March     |9           |2            |\n",
      "|3            |March     |14          |1            |\n",
      "|4            |April     |2           |4            |\n",
      "|4            |April     |3           |3            |\n",
      "|4            |April     |4           |2            |\n",
      "|4            |April     |6           |3            |\n",
      "|4            |April     |7           |2            |\n",
      "|4            |April     |11          |1            |\n",
      "+-------------+----------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fact_orders_by_customer = spark.sql(\"SELECT * FROM fact_orders_by_customer_month\")\n",
    "df_fact_orders_by_customer.printSchema()\n",
    "df_fact_orders_by_customer.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7c34a4-74ab-45cb-b096-fdd860a00012",
   "metadata": {},
   "source": [
    "##### 6.4.4 Create the Final Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "404d6acd-c457-40b0-b77d-c23cfda39c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      33|\n",
      "+--------+\n",
      "\n",
      "+--------+------------+------------+\n",
      "|Month   |Customer Key|Total Orders|\n",
      "+--------+------------+------------+\n",
      "|January |3           |3           |\n",
      "|January |6           |1           |\n",
      "|January |10          |2           |\n",
      "|January |13          |2           |\n",
      "|February|2           |1           |\n",
      "|February|3           |1           |\n",
      "|February|15          |1           |\n",
      "|March   |1           |3           |\n",
      "|March   |4           |1           |\n",
      "|March   |5           |1           |\n",
      "|March   |6           |1           |\n",
      "|March   |8           |4           |\n",
      "|March   |9           |2           |\n",
      "|March   |14          |1           |\n",
      "|April   |2           |4           |\n",
      "|April   |3           |3           |\n",
      "|April   |4           |2           |\n",
      "|April   |6           |3           |\n",
      "|April   |7           |2           |\n",
      "|April   |11          |1           |\n",
      "+--------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Optional sanity check\n",
    "spark.sql(\"SELECT COUNT(*) FROM fact_orders_by_customer_month\").show()\n",
    "\n",
    "# Final formatted selection for customer-level Gold table\n",
    "df_fact_orders_by_customer_gold_final = df_fact_orders_by_customer \\\n",
    "    .select(\n",
    "        col(\"month_name\").alias(\"Month\"),\n",
    "        col(\"customer_key\").alias(\"Customer Key\"),\n",
    "        col(\"orders_placed\").alias(\"Total Orders\")\n",
    "    ) \\\n",
    "    .orderBy(asc(\"month_of_year\"), asc(\"Customer Key\"))\n",
    "\n",
    "# Preview the final selection\n",
    "df_fact_orders_by_customer_gold_final.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c010c836-3593-4ff5-afb1-f5b29798f3dd",
   "metadata": {},
   "source": [
    "##### 6.4.5. Load the Final Results into a New Table and Display the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "3f7ba782-5dca-4030-b386-04e8f7c5e249",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------------+\n",
      "|   Month|Customer Key|Total Orders|\n",
      "+--------+------------+------------+\n",
      "|February|           2|           1|\n",
      "|February|           3|           1|\n",
      "|February|          15|           1|\n",
      "| January|           3|           3|\n",
      "| January|           6|           1|\n",
      "| January|          13|           2|\n",
      "| January|          10|           2|\n",
      "|   April|           2|           4|\n",
      "|   March|           4|           1|\n",
      "|   April|          15|           1|\n",
      "|   March|          14|           1|\n",
      "|   April|          11|           1|\n",
      "|   April|           3|           3|\n",
      "|   March|           5|           1|\n",
      "|   March|           6|           1|\n",
      "|   March|           1|           3|\n",
      "|   April|           6|           3|\n",
      "|   April|          12|           3|\n",
      "|   March|           8|           4|\n",
      "|   April|           7|           2|\n",
      "+--------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save the final Gold-layer results into a managed table\n",
    "df_fact_orders_by_customer_gold_final.write.saveAsTable(\n",
    "    f\"{dest_database}.fact_orders_by_customer_month\",\n",
    "    mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "# Confirm it's saved correctly\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.fact_orders_by_customer_month\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be545e39-4352-4688-b7c1-92097acd2e88",
   "metadata": {},
   "source": [
    "### 7.0. Final Project Wrap-Up & Validation\n",
    "#### 7.1. Validate All Tables Exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "cbb671dd-1607-41d4-8ebd-e1edce43311a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------+\n",
      "| namespace|           tableName|isTemporary|\n",
      "+----------+--------------------+-----------+\n",
      "|rental_dlh|       dim_customers|      false|\n",
      "|rental_dlh|            dim_date|      false|\n",
      "|rental_dlh|        dim_products|      false|\n",
      "|rental_dlh|          dim_stores|      false|\n",
      "|rental_dlh|fact_orders_by_cu...|      false|\n",
      "|          |           customers|       true|\n",
      "|          |fact_orders_by_cu...|       true|\n",
      "|          |              stores|       true|\n",
      "+----------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"USE {dest_database}\")\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4029fc9f-6ed7-48f6-a347-0245926a542f",
   "metadata": {},
   "source": [
    "#### 7.2. Optional: Save Gold Table as Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "02910071-7c3f-4f0c-b7e8-9e2f641499bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output directory for Gold layer if not already defined\n",
    "gold_output_dir = os.path.join(base_dir, \"spark-warehouse\", dest_database, \"fact_orders\", \"gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "2f047ae2-f893-4dea-884b-f2e5ebdd5253",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_orders_by_customer_gold_final.write.mode(\"overwrite\").parquet(\n",
    "    os.path.join(gold_output_dir, \"fact_orders_by_customer_month\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8794a1-1b49-4c01-a87c-8b84cd79c30e",
   "metadata": {},
   "source": [
    "#### 7.3. Create the Silver Layer: Integrate \"Cold-path\" Data & Make Transformations\n",
    "##### 7.3.1. Prepare Role-Playing Dimension Primary and Business Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efd14eb-1c30-4d29-b5ca-46dadafa5419",
   "metadata": {},
   "source": [
    "### 8.0. Stop the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "c6edcbef-aa54-4ca0-aa9f-2b58c8931537",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pysparkenv]",
   "language": "python",
   "name": "conda-env-pysparkenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
